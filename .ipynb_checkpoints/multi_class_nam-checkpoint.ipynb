{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Union, Iterable, Sized, Tuple\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.uniform import Uniform\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import logging\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from absl import app, flags\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncated_normal_(tensor, mean: float = 0., std: float = 1.):\n",
    "    \"\"\"\n",
    "    Initializes a tensor with values from a truncated normal distribution\n",
    "    \"\"\"\n",
    "    size = tensor.shape\n",
    "    tmp = tensor.new_empty(size + (4,)).normal_()\n",
    "    valid = (tmp < 2) & (tmp > -2)\n",
    "    ind = valid.max(-1, keepdim=True)[1]\n",
    "    tensor.data.copy_(tmp.gather(-1, ind).squeeze(-1))\n",
    "    tensor.data.mul_(std).add_(mean)\n",
    "\n",
    "# ActivationLayer Class\n",
    "class ActivationLayer(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Abstract base class for layers with weights and biases\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 in_features: int,\n",
    "                 out_features: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.weight = torch.nn.Parameter(torch.empty((in_features, out_features)))\n",
    "        self.bias = torch.nn.Parameter(torch.empty(in_features))\n",
    "\n",
    "    def forward(self, x):\n",
    "        raise NotImplementedError(\"abstract method called\")\n",
    "\n",
    "\n",
    "class ExULayer(ActivationLayer):\n",
    "    \"\"\"\n",
    "    Custom layer using exponential activation with weight and bias initialization\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 in_features: int,\n",
    "                 out_features: int):\n",
    "        super().__init__(in_features, out_features)\n",
    "        \n",
    "        truncated_normal_(self.weight, mean=4.0, std=0.5)\n",
    "        truncated_normal_(self.bias, std=0.5)\n",
    "        \n",
    "#         self.ExULayer_weights_history = []\n",
    "#         self.ExULayer_bias_history = []\n",
    "\n",
    "    def forward(self, x): \n",
    "        exu = (x - self.bias) @ torch.exp(self.weight)\n",
    "        output = torch.clip(exu, 0, 1)\n",
    "        \n",
    "#         self.ExULayer_weights_history.append(self.weight.clone().detach().cpu().numpy())\n",
    "#         self.ExULayer_bias_history.append(self.bias.clone().detach().cpu().numpy())\n",
    "        if 0:\n",
    "            print('ExULayer_weights:', self.weight.detach().cpu().numpy())\n",
    "            print('ExULayer Normalization L1\\n:', torch.linalg.norm(self.weight.t(), 1, dim=0))\n",
    "            print('ExULayer Normalization L2\\n:',torch.linalg.norm(self.weight.t(), 2, dim=0))\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "class ReLULayer(ActivationLayer):\n",
    "    \"\"\"\n",
    "    Custom layer using ReLU activation with Xavier weight initialization\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 in_features: int,\n",
    "                 out_features: int):\n",
    "        super().__init__(in_features, out_features)\n",
    "        \n",
    "        torch.nn.init.xavier_uniform_(self.weight)\n",
    "        truncated_normal_(self.bias, std=0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = F.relu((x - self.bias) @ self.weight)\n",
    "        \n",
    "        if 0:\n",
    "            print('ReLULayer_weights:', self.weight.detach().cpu().numpy())\n",
    "            print('ReLULayer Normalization L1:\\n', torch.linalg.norm(self.weight.t(), 1, dim=0))\n",
    "            print('ReLULayer Normalization L2:\\n',torch.linalg.norm(self.weight.t(), 2, dim=0))\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "# FeatureNN Class\n",
    "class FeatureNN(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network for individual features\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 shallow_units: int,\n",
    "                 hidden_units: Tuple = (),\n",
    "                 shallow_layer: ActivationLayer = ExULayer,\n",
    "                 hidden_layer: ActivationLayer = ReLULayer,\n",
    "                 dropout: float = .5,\n",
    "                 latent_var_dim: int = 1,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # First (shallow) layer\n",
    "        self.shallow_layer = shallow_layer(1, shallow_units)\n",
    "        \n",
    "        # Hidden layers\n",
    "        self.hidden_layers = torch.nn.ModuleList()\n",
    "        in_units = shallow_units\n",
    "        for out_units in hidden_units:\n",
    "            self.hidden_layers.append(hidden_layer(in_units, out_units))\n",
    "            in_units = out_units  # Update in_units to the output of the last layer\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Output linear layer\n",
    "        self.output_layer = torch.nn.Linear(in_units, latent_var_dim, bias=False)\n",
    "        torch.nn.init.xavier_uniform_(self.output_layer.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        \n",
    "        # Pass through the shallow layer\n",
    "        x = self.shallow_layer(x)\n",
    "        #x = self.dropout(x)\n",
    "        \n",
    "        # Pass through each hidden layer with dropout\n",
    "        for layer in self.hidden_layers:\n",
    "            x = layer(x)\n",
    "            x = self.dropout(x)\n",
    "            \n",
    "        # Final output layer\n",
    "        x = self.output_layer(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "# Neural Additive Model (NAM) Class\n",
    "class NeuralAdditiveModel(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Combines multiple feature networks, each processing one feature, with dropout and bias\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 input_size: int,\n",
    "                 shallow_units: int,\n",
    "                 hidden_units: Tuple = (),\n",
    "                 shallow_layer: ActivationLayer = ExULayer,\n",
    "                 hidden_layer: ActivationLayer = ReLULayer,\n",
    "                 feature_dropout: float = 0.,\n",
    "                 hidden_dropout: float = 0.,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "\n",
    "        if isinstance(shallow_units, list):\n",
    "            assert len(shallow_units) == input_size\n",
    "        elif isinstance(shallow_units, int):\n",
    "            shallow_units = [shallow_units for _ in range(input_size)]\n",
    "\n",
    "        self.feature_nns = torch.nn.ModuleList([\n",
    "            FeatureNN(shallow_units=shallow_units[i],\n",
    "                      hidden_units=hidden_units,\n",
    "                      shallow_layer=shallow_layer,\n",
    "                      hidden_layer=hidden_layer,\n",
    "                      dropout=hidden_dropout)\n",
    "            for i in range(input_size)\n",
    "        ])\n",
    "        self.feature_dropout = torch.nn.Dropout(p=feature_dropout)\n",
    "        self.bias = torch.nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        f_out = torch.cat(self._feature_nns(x), dim=-1)\n",
    "        f_out = self.feature_dropout(f_out)\n",
    "        output = f_out.sum(axis=-1) + self.bias\n",
    "        \n",
    "        if 0:\n",
    "            print('final output', output)\n",
    "            print('f_out', f_out)\n",
    "        return output, f_out\n",
    "\n",
    "    def _feature_nns(self, x):\n",
    "#         for i in range(self.input_size):\n",
    "#             print(self.feature_nns[i](x[:, i]))\n",
    "        return [self.feature_nns[i](x[:, i]) for i in range(self.input_size)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function for regression\n",
    "def feature_loss(fnn_out, lambda_=0.):\n",
    "    return lambda_ * (fnn_out ** 2).sum() / fnn_out.shape[1]\n",
    "\n",
    "def penalized_mse(logits, truth, fnn_out, feature_penalty=0.0):\n",
    "    feat_loss = feature_loss(fnn_out, feature_penalty)\n",
    "    mse_loss = F.mse_loss(logits.view(-1), truth.view(-1))\n",
    "    loss = mse_loss+feat_loss\n",
    "    return loss\n",
    "\n",
    "def l1_penalty(params, l1_lambda):\n",
    "    l1_norm =  torch.stack([torch.linalg.norm(p, 1) for p in params], dim=0).sum()\n",
    "    return l1_lambda*l1_norm\n",
    "\n",
    "def l2_penalty(params, l1_lambda):\n",
    "    l2_norm =  torch.stack([torch.linalg.norm(p, 2) for p in params], dim=0).sum()\n",
    "    return l1_lambda*l2_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(42)\n",
    "    \n",
    "# Generating synthetic data using a normal distribution\n",
    "x1 = torch.normal(0, 1, (1000,))\n",
    "x2 = torch.normal(0, 1, (1000,))\n",
    "x3 = torch.normal(0, 1, (1000,))\n",
    "y = 4 * x1 + x2**2 + x3**3\n",
    "\n",
    "x_data = torch.stack([x1, x2, x3], dim=1)\n",
    "y_data = y + torch.randn_like(y) * 0.1  # Adding some noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-80-fcd7a1a4c170>:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_dataset = TensorDataset(torch.tensor(x_data), torch.tensor(y_data))\n"
     ]
    }
   ],
   "source": [
    "input_size = x_data.shape[-1]\n",
    "shallow_units = 500\n",
    "hidden_units = (300, 100)\n",
    "\n",
    "model = NeuralAdditiveModel(input_size=input_size,\n",
    "                 shallow_units=shallow_units,\n",
    "                 hidden_units=hidden_units,\n",
    "                 shallow_layer = ExULayer,\n",
    "                 hidden_layer = ReLULayer,\n",
    "                 feature_dropout = 0.0,\n",
    "                 hidden_dropout = 0.0,\n",
    "                 )\n",
    "\n",
    "lr=0.4\n",
    "optimizer = torch.optim.AdamW(model.parameters(),\n",
    "                                  lr=lr,\n",
    "                                  weight_decay=0.0\n",
    "                             )\n",
    "#criterion = torch.nn.MSELoss()\n",
    "criterion = penalized_mse\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.5, steps_per_epoch=100, epochs=1000)\n",
    "#StepLR(optimizer, gamma=1, step_size=1)\n",
    "\n",
    "train_dataset = TensorDataset(torch.tensor(x_data), torch.tensor(y_data))\n",
    "train_loader = DataLoader(train_dataset, batch_size=len(train_dataset), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train (100/5000) | loss = 9.82255: 100%|█████████████████████████████████████████████████| 1/1 [00:00<00:00, 14.15it/s]\n",
      "train (200/5000) | loss = 18.39304: 100%|████████████████████████████████████████████████| 1/1 [00:00<00:00, 14.59it/s]\n",
      "train (300/5000) | loss = 8.74731: 100%|█████████████████████████████████████████████████| 1/1 [00:00<00:00, 13.09it/s]\n",
      "train (400/5000) | loss = 8.24451: 100%|█████████████████████████████████████████████████| 1/1 [00:00<00:00, 15.62it/s]\n",
      "train (500/5000) | loss = 16.58724: 100%|████████████████████████████████████████████████| 1/1 [00:00<00:00, 13.76it/s]\n",
      "train (600/5000) | loss = 8.19941: 100%|█████████████████████████████████████████████████| 1/1 [00:00<00:00, 12.81it/s]\n",
      "train (700/5000) | loss = 8.12302: 100%|█████████████████████████████████████████████████| 1/1 [00:00<00:00, 12.25it/s]\n",
      "train (800/5000) | loss = 8.27671: 100%|█████████████████████████████████████████████████| 1/1 [00:00<00:00, 12.36it/s]\n",
      "train (900/5000) | loss = 8.05852: 100%|█████████████████████████████████████████████████| 1/1 [00:00<00:00, 12.13it/s]\n",
      "train (1000/5000) | loss = 9.02194: 100%|████████████████████████████████████████████████| 1/1 [00:00<00:00, 12.86it/s]\n",
      "train (1100/5000) | loss = 8.15972: 100%|████████████████████████████████████████████████| 1/1 [00:00<00:00, 11.23it/s]\n",
      "train (1200/5000) | loss = 8.01976: 100%|████████████████████████████████████████████████| 1/1 [00:00<00:00, 11.16it/s]\n",
      "train (1300/5000) | loss = 8.43254: 100%|████████████████████████████████████████████████| 1/1 [00:00<00:00, 11.88it/s]\n",
      "train (1400/5000) | loss = 8.07197: 100%|████████████████████████████████████████████████| 1/1 [00:00<00:00, 11.45it/s]\n",
      "train (1500/5000) | loss = 10.32917: 100%|███████████████████████████████████████████████| 1/1 [00:00<00:00, 11.64it/s]\n",
      "train (1600/5000) | loss = 8.17311: 100%|████████████████████████████████████████████████| 1/1 [00:00<00:00, 15.12it/s]\n",
      "train (1700/5000) | loss = 12.82856: 100%|███████████████████████████████████████████████| 1/1 [00:00<00:00, 13.03it/s]\n",
      "train (1800/5000) | loss = 8.87492: 100%|████████████████████████████████████████████████| 1/1 [00:00<00:00, 15.18it/s]\n",
      "train (1900/5000) | loss = 21.59301: 100%|███████████████████████████████████████████████| 1/1 [00:00<00:00, 13.05it/s]\n",
      "train (2000/5000) | loss = 13.62274: 100%|███████████████████████████████████████████████| 1/1 [00:00<00:00, 15.36it/s]\n",
      "train (2100/5000) | loss = 11.35124: 100%|███████████████████████████████████████████████| 1/1 [00:00<00:00, 15.30it/s]\n",
      "train (2200/5000) | loss = 10.32230: 100%|███████████████████████████████████████████████| 1/1 [00:00<00:00, 12.52it/s]\n",
      "train (2300/5000) | loss = 9.83629: 100%|████████████████████████████████████████████████| 1/1 [00:00<00:00, 11.17it/s]\n",
      "train (2400/5000) | loss = 9.48815: 100%|████████████████████████████████████████████████| 1/1 [00:00<00:00, 14.62it/s]\n",
      "train (2500/5000) | loss = 9.42085: 100%|████████████████████████████████████████████████| 1/1 [00:00<00:00, 13.80it/s]\n",
      "train (2600/5000) | loss = 9.50642: 100%|████████████████████████████████████████████████| 1/1 [00:00<00:00, 12.29it/s]\n",
      "train (2700/5000) | loss = 9.29218: 100%|████████████████████████████████████████████████| 1/1 [00:00<00:00, 10.47it/s]\n",
      "train (2800/5000) | loss = 9.20800: 100%|████████████████████████████████████████████████| 1/1 [00:00<00:00, 10.72it/s]\n",
      "train (2900/5000) | loss = 9.27089: 100%|████████████████████████████████████████████████| 1/1 [00:00<00:00, 13.23it/s]\n",
      "train (3000/5000) | loss = 9.17696: 100%|████████████████████████████████████████████████| 1/1 [00:00<00:00, 15.85it/s]\n",
      "train (3100/5000) | loss = 10362.35938: 100%|████████████████████████████████████████████| 1/1 [00:00<00:00,  9.03it/s]\n",
      "train (3200/5000) | loss = 53.93928: 100%|███████████████████████████████████████████████| 1/1 [00:00<00:00, 13.21it/s]\n",
      "train (3300/5000) | loss = 50.99297: 100%|███████████████████████████████████████████████| 1/1 [00:00<00:00, 14.12it/s]\n",
      "train (3400/5000) | loss = 48.26241: 100%|███████████████████████████████████████████████| 1/1 [00:00<00:00, 15.22it/s]\n",
      "train (3500/5000) | loss = 45.74902: 100%|███████████████████████████████████████████████| 1/1 [00:00<00:00, 13.72it/s]\n",
      "train (3600/5000) | loss = 43.46452: 100%|███████████████████████████████████████████████| 1/1 [00:00<00:00, 14.40it/s]\n",
      "train (3700/5000) | loss = 41.41619: 100%|███████████████████████████████████████████████| 1/1 [00:00<00:00, 15.78it/s]\n",
      "train (3800/5000) | loss = 39.60637: 100%|███████████████████████████████████████████████| 1/1 [00:00<00:00, 11.01it/s]\n",
      "train (3900/5000) | loss = 38.03238: 100%|███████████████████████████████████████████████| 1/1 [00:00<00:00, 12.68it/s]\n",
      "train (4000/5000) | loss = 36.68661: 100%|███████████████████████████████████████████████| 1/1 [00:00<00:00, 12.74it/s]\n",
      "train (4100/5000) | loss = 35.55690: 100%|███████████████████████████████████████████████| 1/1 [00:00<00:00, 12.73it/s]\n",
      "train (4200/5000) | loss = 34.62713: 100%|███████████████████████████████████████████████| 1/1 [00:00<00:00, 12.69it/s]\n",
      "train (4300/5000) | loss = 33.87806: 100%|███████████████████████████████████████████████| 1/1 [00:00<00:00, 13.60it/s]\n",
      "train (4400/5000) | loss = 33.28826: 100%|███████████████████████████████████████████████| 1/1 [00:00<00:00, 13.95it/s]\n",
      "train (4500/5000) | loss = 32.83522: 100%|███████████████████████████████████████████████| 1/1 [00:00<00:00, 13.40it/s]\n",
      "train (4600/5000) | loss = 32.49639: 100%|███████████████████████████████████████████████| 1/1 [00:00<00:00, 10.96it/s]\n",
      "train (4700/5000) | loss = 32.25015: 100%|███████████████████████████████████████████████| 1/1 [00:00<00:00, 12.49it/s]\n",
      "train (4800/5000) | loss = 32.07666: 100%|███████████████████████████████████████████████| 1/1 [00:00<00:00, 11.06it/s]\n",
      "train (4900/5000) | loss = 31.95843: 100%|███████████████████████████████████████████████| 1/1 [00:00<00:00, 11.20it/s]\n",
      "train (5000/5000) | loss = 31.88071: 100%|███████████████████████████████████████████████| 1/1 [00:00<00:00, 12.36it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAskAAAFpCAYAAABuwbWeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZgcVb3/8c93tkz2fSUhCyEhAZIAMSxhD2BYFBUXUBQVjXpBUbxXgau44FUuKnpRlh8Kgogg9wqKEPYtLGFJQiA72ZPJnklmkslk9vP7Y3omPTXdMz29VFV3v1/Pkyfd1dV9TnfVdH/q1KlzzDknAAAAAIcUBF0BAAAAIGwIyQAAAIAHIRkAAADwICQDAAAAHoRkAAAAwIOQDAAAAHh0GpLNbJSZvWRmK8xsmZldE1k+wMyeM7PVkf/7x3n+bDNbZWZrzOy6dL8BAAAAIN2ss3GSzWy4pOHOuUVm1lvSQkkfk/RFSXucczdHwm9/59z3Pc8tlPSBpHMllUl6R9JlzrnlaX8nAAAAQJp02pLsnNvmnFsUub1f0gpJh0m6WNL9kdXuV3Nw9pohaY1zbp1zrk7Sw5HnAQAAAKHVpT7JZjZG0nGS3pI01Dm3TWoO0pKGxHjKYZI2R90viywDAAAAQqso0RXNrJekv0v6tnNun5kl9LQYy2L27zCzOZLmSFLPnj1POOqooxKtWlbYWF6tfTX1kqRjD+sbcG2kJVsqJUlHj+ijgsS2ZUrlTBreR0UFmSvHq7yqVlsrayQF93m3vPeJw3qrpND/a2Rbyk/n+295zRapvPamPdWqPFivbkUFmjC0d5fKHzuop3p1S/jrK69t3lOtioP1GtW/u/r1KAm6OhlTU9+o1TurJIXjOxZAdli4cOFu59zgWI8l9CtjZsVqDsgPOucejSzeYWbDnXPbIv2Wd8Z4apmkUVH3R0raGqsM59zdku6WpOnTp7sFCxYkUrWs8ZX7F+j5FTskSQtuvjDg2khjrntSkvTqTz6snhkMGy3lvPjDczWgp38/0Pe9vl4//ldz1/egPu+W9z73+2dpZP8egZWfzvff8potUnntqx5cpCeXbNP4Ib30/LVndKn8u788Q6dPiPmdBo9rHn5X/1y8Vb/6zDR97LjcPZG3cvs+zf7tq5LC8R0LIDuY2cZ4jyUyuoVJukfSCufcrVEPPS7pisjtKyT9M8bT35F0pJmNNbMSSZdGngfkjR37aoKuQs644NhhkqSGpqaAawIAyHWJnAOeKenzks42s8WRfxdIulnSuWa2Ws2jV9wsSWY2wszmSpJzrkHS1ZKeUfMFf48455Zl4H0gSR2PbZK9wvS+vvbAoqCrkDP+7czxkqQv35dbZ5oAAOHT6Xl259xrit23WJJmxVh/q6QLou7PlTQ32QrmjjDFNv91NtRgLjtQ2xB0FXJGUaF//doBAPmNK18AZA0/L/5Edsnj43AgZfX19SorK1NNTe52DywtLdXIkSNVXFyc8HMIyXkun1t4/eLy/CxCOhUV+D9KCADkurKyMvXu3VtjxoxRgqOXZRXnnMrLy1VWVqaxY8cm/Dx+cYAM4zgkfXLwuxsAAldTU6OBAwfmZECWJDPTwIEDu9xSTkgGkDXqGxnVAgAyIVcDcotk3h8h2SfRrYlh6uLgV03C846RzQb3Lg26CgCADOjVq1fQVWiHkByAuUu2B10F+IgDhPTp271Yp08YrKmj+gVdlaxD33gA6BpCsk+iW/nLD9QGV5E8EaLGelJymhWatNQzPTYQqr95AGmxePFinXTSSZoyZYo+/vGPa+/evZKk2267TZMnT9aUKVN06aWXSpJeeeUVTZs2TdOmTdNxxx2n/fv3p1w+o1vkOX5YkG1eWrVLkrRq+35NHNY74NpkD4s73D0AHPKTfy3T8q370vqak0f00Y8+cnSXn/eFL3xBv/vd73TGGWfoxhtv1E9+8hP99re/1c0336z169erW7duqqiokCT96le/0u23366ZM2eqqqpKpaWpd8+jJdknhNE8RjaJKdXT/7urOCPTFXS3AJBNKisrVVFRoTPOOEOSdMUVV2jevHmSpClTpuhzn/uc/vKXv6ioqLm9d+bMmbr22mt12223qaKionV5KmhJ9km+/zzl9UFCPr/3BCR7DMHEIgCQfsm0+PrtySef1Lx58/T444/rpptu0rJly3Tdddfpwgsv1Ny5c3XSSSfp+eef11FHHZVSObQk5zsCXMbRgpcZxUV8fXUF3S0AZJO+ffuqf//+evXVVyVJDzzwgM444ww1NTVp8+bNOuuss3TLLbeooqJCVVVVWrt2rY499lh9//vf1/Tp07Vy5cqU60BLMoCsUlJUoLqGpvw+O5EEDtYAhFl1dbVGjhzZev/aa6/V/fffr69//euqrq7WuHHj9Kc//UmNjY26/PLLVVlZKeecvvOd76hfv3764Q9/qJdeekmFhYWaPHmyzj///JTrREgGEIhkQ+49V0zX5+95m4lF0AYHAUB2a2qK/Z3+5ptvtlv22muvtVv2u9/9Lu114nylT6InEOGkZ36hxbNjXZ0EqVtRoSSpoZEPtivobgEAXUNI9klYf879an3xu5UnrJ83UldU2Bz26uO0OiC2MLS0zl9brsWbK4KuBgAkhO4WQIYFH01yS3FB87E9LcnZ57I/NJ823XDzhQHXBAA6R0sykGGO/hZpVVzU3JL8zoY9AdcEAHJHrv9WJfP+CMl5Lsf/JvJWLn/ZFUVaku+ety7gmgBAbigtLVV5eXnO/nY451ReXt7lWfjobuGTHN3vElbPqXGkSa5+iWdarl+4x24BJG/kyJEqKyvTrl27gq5KxpSWlrYZYi4RhGT44vpHl+jPX54RdDWQA/p2Lw66ClkpDBfuAQin4uJijR07NuhqhA4h2Sdh/Xnyq17zPsjdo9POBLHtc7lVbUifUo0Z2ENjBvUMuioAgBxGn2TkpDCdkg9RVUIllc9lYK9ujG7RRbne3QIA0o2Q7JMwhTbkvmza25IJb8WFpjpm3OsSulsAQNcQkvMc4T3zujqjHDrX5KRN5dVBVwMAkMPokxwEUlNeCeI4JNcPft5e3zxGsnNOxt9TQuhuAQBdQ0sygKzV0JTbBwPpRHcLAOgaQnKe42czN+XLdq2nXzIAIEMIyQHI9VPhgF/qG/hbAgBkBiHZJ9G5+JfPrAquIkAO+N7siZLECBcJoCcyACSHkByA/TUNQVehFY3auSnXt+uAHiWSpIYmQnJncnxXAICMISQDCESPboWSpOH9Srv83OLC5q8uulugRa4fGALwHyHZJ1xZDj9lw/42eXgfSdIvPnFsl5+7t7pOkvTU0m1prVMuorsFACSn05BsZvea2U4zWxq17G9mtjjyb4OZLY7z3A1mtiSy3oJ0Vjzb0MqBXPTzuStSfo2e3bo+XPuq7fslSfe/sSHl8nMdXz0AkJxEWpLvkzQ7eoFz7jPOuWnOuWmS/i7p0Q6ef1Zk3enJVxOZkg0tjug6vw7K7p63zp+CPAoiE4g0cvQJAMiQTptwnHPzzGxMrMeseaqrT0s6O73VAoD4WvozFzLbXqf4hAAgOan2ST5N0g7n3Oo4jztJz5rZQjOb09ELmdkcM1tgZgt27dqVYrXChwYvILZkQtw3zjxCknTZjMPTW5kcxFcPACQn1ZB8maSHOnh8pnPueEnnS7rKzE6Pt6Jz7m7n3HTn3PTBgwenWK3wCW23hpBWC+hI3+7FkqSCAtpJ0Sy037EAslbSIdnMiiR9QtLf4q3jnNsa+X+npMckzUi2PKAraLnPbcUFkSHgmEwEAJAhqbQknyNppXOuLNaDZtbTzHq33JZ0nqSlsdbNB0bPQPgo1w8SWlqQf/t8vJ5eAACkJpEh4B6SNF/SRDMrM7MrIw9dKk9XCzMbYWZzI3eHSnrNzN6T9LakJ51zT6ev6tklrKcCw1krAOnC4TkAJCeR0S0ui7P8izGWbZV0QeT2OklTU6xfzsj1lr2wyfdBD8J6UAb/sScAQHKYcQ85iYMSAACQCkJyniNM5qZ82K79ehSrkNEtOpUvn1A+7PMA/EVI9gnf30B6nT1xiIb3LQ26GqHHdw8AJIeQ7Bd+qYC06lZcoNoGhoBDW7dcMiXoKgDIEYTkPJerF3jl6vtKVD68+25FhaojJHcqX7pbtBjUuyToKgDIEYRkn+R7aAPSrbjQVHmwXgfrGoOuSqjxzQMAySEkIyfl++QtLguuYkq1iv17NrcY7qupT0NtkO3Cv8cDyDaEZJ+ENbRlQZZKCi332cOSHNR6UM9ukpiaujPh/ObJnLB+1wLIPoRkn4Q1tO2uqg26CsiATOxty7ZWZuBVk9cy/Nu+gw0B1wQAkIsIyT4Ja4vtJ+54I+gqIEu8s35P0FVoY97qXZKknz25POCaZIcFG/YGXQUAyCqE5DzX0BTS9I6URB+U1dRn9sK2HftqMvr68VRHLtjbW02f5EQ8+NamoKsAAFmFkIycFNaW+yA89u6WjL5+pkN4Z+iBCgDIBEKyT8hsyHbx9uGgDkjGDeopSRrGrHtQdozoAiC7EJJ9whc4fOXj7hbUnv3lU8dKkvp1Lw6oBgglTi0ASBNCMoCExMseQR0AFhc2f309muHuJACA/ERIRk7K93b76CEHM51hg7r2s4AWQwBABhGSfZLvoc1v9G7xUzAfdkkRX18AgMwpCroCANLPz4OEoA5IepQUqajAdObEwcFUAACQ02iKQU4K6wyHuSjIT7qhyen5FTsDrAHCgr94AOlGSAZykJ+BIdmWZA5kkAl0VQeQLoRkn9BH1l983v5JNeymEmrGD+mVUtkAAMRDSPYJmQ1ByXSLbVNTRl++Q2t2VkmS9hyoC64SAICcREgGcpCfYxeHodtERTUhGQCQXoRk5CRmOPRPGD7qm59aGXQVAAA5hpDslzAkCeSNfNnbjju8nySpuq4x4JogaHzFAkg3QjJyEj+Y6RfvIw3ysy6w5sv+3tmwJ7hKIFTMGN8CQHoQkn1CZoOf/Ayuuw/U+leYx9A+3SRJtQ0BXj0IAMhJhGTkJA5K/POlP70TWNnfPW9iYGUDAHIbIRk5ie4W+aGkkK8wAEBm8AsD5KBMDMsWxp6ehQVhrBUAIBcQkn1Cy6a/wjB2LzJvRL/urbebmtjm+Y3tDyC9CMk+IbTBV3m4u/3q2VVBVwEhwLkFAOnSaUg2s3vNbKeZLY1a9mMz22JmiyP/Lojz3NlmtsrM1pjZdemsONCR6Jb76rqG4CoSAmE9i5Huet3x8tr0vmAO2sv03QCQsERaku+TNDvG8t8456ZF/s31PmhmhZJul3S+pMmSLjOzyalUFkjGm+vKg66C70Kai2NiWFv/lO09GHQVACBrdBqSnXPzJCUzUv8MSWucc+ucc3WSHpZ0cRKvA3RZdEgMa0sq0mPUgO6drwQAQBel0if5ajN7P9Ido3+Mxw+TtDnqfllkWUxmNsfMFpjZgl27dqVQrXAiqPkszz/wfHr7t356WuvtXfuDm9gkGwR5bYTLp50SQE5INiTfKekISdMkbZP06xjrxDqJGvdb0jl3t3NuunNu+uDBg5OsVnjx+4BsF9ZdeGDPktbb97+xIbiKZIEgv4cyXTbfsQDSLamQ7Jzb4ZxrdM41SfqDmrtWeJVJGhV1f6SkrcmUB3QVv5f5Y9zgXq236xuZnjqs/PqbpI87gHRJKiSb2fCoux+XtDTGau9IOtLMxppZiaRLJT2eTHlAKvKxhSlfhxysIySHFt0tAGSbos5WMLOHJJ0paZCZlUn6kaQzzWyamhsHNkj6WmTdEZL+6Jy7wDnXYGZXS3pGUqGke51zyzLyLgAPfo/zy6Be3bS7qlY19Y1BVwVx8CcJINskMrrFZc654c65YufcSOfcPc65zzvnjnXOTXHOfdQ5ty2y7lbn3AVRz53rnJvgnDvCOfdfmXwjYef9gVi4MZkBQ5CofG1JbRF9kJAPn8Qfr5guSXro7c2drJnf8mFfAIB0Yca9gGwsrw66CkDOKC3mqyzsOLsDINvwy+IT+uP5K99aUr38fs9Bz2o4PuriPYRTps/u5OPfOYDMIiQDSNm8D4Id27yokK+ysPOrncBijj4KAF3HLwtyUr63KmXizEVH0aMp3z/wLMEZLQBIHCEZOYks4K+mJD7wTG2ildv3ZeiVkQr+JgFkG0KyT/iBQC5LZf9O1+nxuz9/giTpWw+9m5bXQ3rl+4gzALIPIRk5Kd9/kP0+KEumJTndiouav84+2FEVcE3CK8itFIJdBAC6hJDsE29o4wfDPx/s2B90FXJeGPbn0QN6BF0FdCDTu0gY9kEAuYWQjNwU9YP5y2dWBVePHOLNIGbRjwWfUMYxDBzUdr8EgFQQkgOyaQ+TiSC7ff2MI1pvNzUFWJEYmhhuI3QYWQNAtiEkB+R/XlgddBVyWr7/HLfJIxkKJxOH9m69HYY+yZI0YWhza/IHO+liE0uQmym66IN1jYHVAwASRUhGTqLVKvN6dStqvR2WT/u684+SJK3deSDgmoTT/pr6wMqO/pP8zt8WB1YPAEgUIdknZDbksrAclJw0bqAkafHmvQHXJDwsqpPu1x5YGFxFonaRJVsq0//yIdkHAeQOQrJP+Pr2V77/XvpxIV10CWHpAtyjpLl1+w+vrg+4JuERHR5rG4LrPO7XxZ1ctwcgXQjJAJISln7I8dQ20O81TEK+uwBAO4Rk5KSw/R5v2O1vH1k/AknYQ89qJhWR1La7RZBCvrsAQDuEZJ/QX85fYfu41+0OLrBl7qMI2YcccdmMUZKku15ZG3BNwoHvHgBIDiHZJ/xMwU/R+9v2yprMlJHiTp2p7PbNs4+UJD3x/rbMFICkENYBZBtCMnJSGGaAC4s7Xs5Mi2q6LtZLd2+AEf26p/cFkRYZn5Y6w68PIP8Qkn0Sjl6BCEq1z5Mn+NFqF+YL91rGcKb1Mjx82xR82QJIE0KyT/ip9lfYstF/PrY06CqkzJs9fJjUL2nfj0wqUrb3YMA1CV54LtwL2U4CAJ0gJPuF34e8VnkwuJnO0sW7C4e5lfaIQT0lSc+v2BFwTYIXmu0UkmoAQKIIyUAO8iOPRGevsLUSHj+6vyTp9y+uCbgmaBGuPQQAOkdIRk4KTetZDgtzn+TS4kJJUvmBuoBrErzQdLeI2l1CUiUA6BAh2SfhjRPIRUwmIg3vWyopc0PgZYuwHDBm+mxDSN4mgBxCSEZO4vcyv0e3kKSPThshSXri/a0B1wRemdx1jOEtAKQJIdknYWnNCVJVbUPQVcgbvrQkZ76IlHx71gRJ0u6q/O5yEcbuFgCQDQjJPuH3QZrr4wxo+f6D7Pfb31oRvqHWupcUqrS4IO+npw7LAXo4agEAiSMkwzd+joAQttEW/OZHLvro1BGtt29/KZxBdECPEklSU7qmB0TSwhLWASBRhGQgB/lxkNAygkSY/fuHJ0qSFm7aG3BNghPG7hYhqRIAdIiQDN/42ZCU741W2fD+/QjyZ00cIkl6c215xstCsPL97BGA9CMk+yQbQgtyB/tbs/49SzR1ZF+9uGpn0FXJe37tk7RSA0iXTkOymd1rZjvNbGnUsl+a2Uoze9/MHjOzfnGeu8HMlpjZYjNbkM6KI/v4mdvyPSPSqnbIrElDtXhzhXbtrw26KnmNfRJAtkmkJfk+SbM9y56TdIxzboqkDyRd38Hzz3LOTXPOTU+uisgVtG76h8/6kLMmDpFz0utrdgddlbzGPgkg23Qakp1z8yTt8Sx71jnXMujtm5JGZqBuyDG+jm7BD3JGnTh2gCSpqCD857Ynj+ijQb1K9NyKHUFXBQCQRdLRJ/nLkp6K85iT9KyZLTSzOR29iJnNMbMFZrZg165daahWuHCq0e/gmt+fd6Znw/vaGeMkSQ9+5cSMlpMOhQWmcycP00srd6qmvjHo6uSt/P6LBJCNUgrJZvafkhokPRhnlZnOueMlnS/pKjM7Pd5rOefuds5Nd85NHzx4cCrVCqUwt2w25sEYshvLDwRdBV9len9rmfp3RL/umS0oTc4/Zpiq6xrpchGg6HGSM3JxXe5/jQHwWdIh2cyukHSRpM+5OKPEO+e2Rv7fKekxSTOSLQ/Zz9cL9zyF7diXXxdt+fVZR4edME87ftK4gepdWqSnl24Puip5y7d90qdyAOS+pEKymc2W9H1JH3XOVcdZp6eZ9W65Lek8SUtjrZsPwtySnA8zYeXbsFB+bdPoiSruDvH0zyVFBTp30lA9u3yH6hqagq5OXoreJfPgKwdADkhkCLiHJM2XNNHMyszsSkm/l9Rb0nOR4d3uiqw7wszmRp46VNJrZvaepLclPemcezoj7wLZwcdfRm9ReZaRfWu1i57uuTHkyefCKcNVebBer67OvWseskO49w8A8CrqbAXn3GUxFt8TZ92tki6I3F4naWpKtYMv/Prp8nec5Lal1TXmV+uhX3m1ISokh71r+2lHDlb/HsV69N0tmjVpaNDVCUxFdZ369SjxvdzofbLyYL3v5QNAVzHjHnwTZEPjz+euCK7wQPjzYTdEHXyEvCFZJUUF+thxh+nZZdu150Bd0NUJzDPLgumXHX0Qtb8mvP3XAaAFIRm+hRs/+z57i1q/i9EtMiG6i0U2DHP4mQ+NUn2j02Pvbgm6KoGxgDofZXpYwvDvfQCyDSEZecHy7Mq9jAeGyMc5YUjvQ2V2sdAgWp6PGtZHU0f10yPvbM6LC1bDJNMhuUW+/a0DyBxCsk/C/IPsVwvgbS+u8aUcqX1IzLefTb92t4KoGfeS3cf9zjSfmT5Kq3bs19vr93S+ci4K6I8hxF+BABATIRm+8bMfaL7/IAdxUJYtn/nF00aoX49i3fPa+qCrkle8+8e2yoPBVAQAEkRI9kmY80O2hBskzs9N2tISnC27Uc9uRfrMh0bp2eU7tH53fvVVl4I7q+LtbnHHS+EdVxsAJEJyoA7WNQZdhZyVDReRZZKfBz6FkZScTQdbV5w8RpL04Jsbg61IAILqs+sNyXQdBhB2hGSfxAoQX7rvbf8rkqfy7QfZz4OEln7Jfl2YlQ4j+nXX7KOH6bF3t6imnoNVP3jH0U73n2QW7X4AsgQhOUBvrsvTC4f84P1Bzr+U7JuWa/eybezhK04Zo/IDdbr/jQ1BV8VXQf0l+DdVui/FAMgDhGTQApOD/NykLd0tHn9vq4+lpu7kIwbq1PGD9IdX1+VV16fVO6sCKbddSzJpFkDIEZKRk7whMd+mwfXzwCebw843zx6v3VV1eujtTUFXxTd3vRLMBXPZ1B0HACRCMnJUmMel9oOffZKz+bM+cdxAzRg7QD99YrnKq2qDrk5O8+4mdVFTmgNAGBGSfRLm0RYamvixyjV+5tbw7tmJuWbWkZKkL/7pnYBrktu8B1N/fSt/Wu8BZCdCsk/C3Nj2m+dWB12FtAv64w66ddXP0qNPo1fXNfhYcnrMHD9IkrRkS6WeWrIt4NrkLm+f5HQLc0MEgOxESIYWbmSUjVzjZ0gviOqT/JPHl/tWbjr979dPliR948FFqmvIjTMru6tqdcmdb2hLRThmtvOrT3L29pAHEDaEZOTkmElBt9wHXr6PZT3ytZNbb2/fV+NjyenzoTED1KtbkSTps394M+DapMff3tmshRv36u314TgI5sI9ANmGkOyTUP885OCPV9DvKFb5+2p8HGHDxw/gmMP6tt5OJghZSNr+3rphliRpwca9OTHBSH3ILozLwa8ZADmOkOwTfiDwsdtf962soPpnNjRm747es1uRbrxosiTpwRy4qKwgZGeIaEkGkG0IyQEL+gIvSTna3SLgC+dilL/Vx76hQb39sLVedtWXZo7RxKG99Yd561RZnV9ja2darH2yMY1X84XhqxRAbiEkB+wvb24MugrIQd7A8OLKHb6Um+1j35qZfvmpKdqxv0a/enZV0NVJSUctt7UN/ncniVWfTNQjB4/5AQSEkOyTaaP6xVz+r/eDH3Lqvc0VQVch7YJuVApb+V++b4Ev5dbWZ3dIlqQpI/vpS6eM1QNvbtRzy/05uMiEpg5aaW/8xzIfa9Is00PAAUC6EZJ90rd7cdBVgI+CPvUbVHeTbG9JbvH98yfqmMP66D/+770ud5MJuqtPi4YOUuniAA6Mw/K5AECiCMkBy+Uzg4H+KIbw97jGx1bWoFrt1u8+EEzBadatqFC/u+x41Tc06aq/Lkp4tIvGJqdTbn5Rt70Q/AQ9jR38/e33c6SViFj7JLkZQJgRkpGTgp59K+jyM32UMG1k2+5D4wb1zGh5QRg7qKd+cckUvbupQjc9sVxXPbhIP/jHkg6fs+9gvbZV1ujW5z7wqZbxdTTSyIG6cPRJ3pGl42oDyA+EZGRMPrcSBf3eM11+/54lbe7H63Of7T46dYTmnD5OD761SU8u2aa/vLmpwzMkVbXhmZa7o4vigmlJbv+5fftvi9P2+nn8dQMgQwjJSVqzs0o/n7si4SGMgm9ZDAe/umAEHVKD5vfbLy0p9LlE/3z3vAmaMLRX6/1HFmyOu24mJox5dtl2nfHLl7o8vF5H02sH0R0n1t9kZiZtyeVObAD8REhO0s+eXK67563T+2WxL4DZVF6ts3/1spZv3dfh67wVwJSxvgXVGMt+8q/lvpQdSz5dOOT3Wx3Vv0fr7YYcuXivRbeiQv3lKyfq62ccIUn6/t+XxJ26uqom/S3J1z+6RBvLq7W3uq5Lz+soJAchVkNBHv1JAshChOQk7TvY3GIUryX5mWXbtW73AT3+3lY/q5UQv1qRYoXS+97Y4FPZ7Zc9s2y7L2XHK99PsQJJJqda/uppYw+VE7Jwlg5Depfqa6ePa73/xtpyLdzY/gB3f4Ihua6hKeFW55YRQ7o6vF5nI434PfV2U4zqkJEBhBkhOUXxvuRb+t+FcSrWWHXKRCtrkO88Vkgs2+vfjHdBi7U5M7kvFhUe+iqpTTB8ZVvLfv+eJXr7hlmt9y+5c75eW71bv3xmpb798LuS2vZJfmdD/LNEV/91kU7775cSKrelm8XBOJ/rmp1VMZd31pJ897x1CZWfLn597wBAuhCSUxSvJbll+KUwzv4U+8cq/eXk8+9f0H3QY5Xu1/bo6lTDYfwbiWdInwslaIIAACAASURBVFKt/q/zW+8//M4m3f7SWv1j8VbtrqpVRVSXiM/f81bc13l2+Q5VHqxvd3Hds8u2t5vApD4ySsWBGBcFPvT2Jp1z6ysxA3ltJyF5z4Gudd9IVaz9b+2u3BgyEEBuIiSnKN4wSy2zXRW2JIAOcsOq7fvTXa0OxfqxykR+CrIVPeZ79LE6QR8gxGqh+7+FZb6UnYmL18KkuLBAT11zmk4dP0hPRM2Y+d7mijbBM5FxsQ/Utg3Jcx5YqK/+ue3siC0HHQdjDNu2aONeSbFbkzsLyX7L9PcBrdIA0q3TkGxm95rZTjNbGrVsgJk9Z2arI//3j/Pc2Wa2yszWmNl16ax4WMS74rylMa2woDkkd/T1/dKqnWmuVcdiB8jc/4HpaHKFfLB2V+zT8ulyzGF9JAV7caZfJg3vox9cNEmnHTlIl35olCRpyZZKlXtaZzsbEq4rF/pVxwjJHe3TnXW38Ptv3q9rIbLpzASAcEukJfk+SbM9y66T9IJz7khJL0Tut2FmhZJul3S+pMmSLjOzySnVNoTiheSW1h+LfGOHqW9yrLqsy8BMaYG2JMdYdvNTK30beSHorR3ER/+RKSMkSa+u3u1/4QE4algfPXDlibr5kika0LNEv31+tR58a1Obde57fX2750VfMBfd6h69bzbFSJSxLsRraS2OdRFeZyH5/vkbO3w83RpjXbkHACHWaUh2zs2T5O3wdrGk+yO375f0sRhPnSFpjXNunXOuTtLDkeflhJbfsHhBsKWVpqW7RUehxe9AE6vO5/1mXgbKSftLpmxrRX7M8BXEcFvxLizLBy2tyV6/evaDdoE3urU5unU4uqvG/hgt0LFCb8uIF7FamTsb3UKSdlfVdrpOutTH6ZpWWZ3b3XMAZK9k+yQPdc5tk6TI/0NirHOYpOhR98siy3JCSwiO9zvUchq05aL/jvLJa2t2pbFmnQtyCDi/xCvar1OxQXdfiVX8A29mtuUwn0Py92YfpXuumK5xg9tPz33x7a+3CbjlUcH00/9vfmsr8O6q6PDcHJKjW4hjzaDX8ngyLcmStDbOyBiZEO+s29SfPutbHQCgKzJ54V6sOBI3OZjZHDNbYGYLdu3yNzQmoyUEx+sT2PJ7YK0tyfFD0+trytNbuU74FeCCbUmOXbhvITnO8kz3C24tP4DPvjFOS2G+mDVpqF787pm6/KTD2yxfsqVSl9z5RuuFd+VVbfstbyyvltS2VbelZbhlPHapeRIT70WRFQebX+uppdu1sbxtl6mwTSbSEMZTSwDQgWRD8g4zGy5Jkf9jXXlWJin6HORISXFn1nDO3e2cm+6cmz548OAkq+Wflu51sfoOSoeCaOvgFiH6ffCtLgG+5/gtycFe1ePXBA5BfPRXnz2+9faOffnRrSWWH1w4WT/72DFtli3ZUqlJNz6tD/9mnrZ7Ppur/rpIjU2uzfJZv35F5VW1qjzYNhT/wTO28d4DzY+v2VmlM375cpvHEulu8Zm7Y88cmAmZDu0h+ooFkCOSDcmPS7oicvsKSf+Msc47ko40s7FmViLp0sjzckJTa3eLOEPARR5vCWtBj5sbza8L6uKVs2Zn5oe8i1d2vIOadAv6oCiI7h79epT4XmYYlRYX6vKTRmvJj89r99iqHft172ttL+Zbs7NKzy7bru2VbcPz62vL241l7N2vvSE6Wqwh44LU4NOFewxuASBdEhkC7iFJ8yVNNLMyM7tS0s2SzjWz1ZLOjdyXmY0ws7mS5JxrkHS1pGckrZD0iHNuWWbehv+aEuxu0RKiO8ssjyzY3PEKaeTXWc94QXWLDxfPxXuPXZ3oIlnxQur8tf50rQn6kCxsp/qD0Lu0WOt+foH++tUT2yxfvbNKl81oe6HfNx5cpEcXtR3H+pml27Xa02f47fV7WvetuoamuEPMNTa5hPuI+9UFqL7RqXtxYczHfvx4zvw0AMghiYxucZlzbrhzrtg5N9I5d49zrtw5N8s5d2Tk/z2Rdbc65y6Ieu5c59wE59wRzrn/yuQb8Vvr6BbxZtyLtJo0JBiSv/d/76etbp3xq5UxXil+lB8voPvVmhUvi9/3xgZfyg86JX+ww98JcsKqoMB0yhGDtOKns/XYv52iScP7aGifbrru/Ent1t1QXq3S4kNfyU8u2aYf/GNpm3Xe2bC3dQKT6Nn9vFou/Is2vG9pzHVn/fqVhN5LquoamlRcGLud17e/CwDoAmbcS1JnLcn1kZTUEqLD1d3Cr3LC855bnHPrPK3PwJjQXvHeu38XDnbcVz5TuhU1f6Vcef+CTtbML91LCnXc4f31t6+dpKevOV19uxfHXG/KyH7tlkUHZ0n65kPvan9NfbuJSyTpf55fLan9TH6S9L3ZE+PWr+V5mdTQ1KTiQn5yAGQPvrGS1OQJwV71kdPNLSE6kWDqXwtv7HIWRqa4TWNBMcWbyjudOgro75dVBFa++dRjMt7bz/Qudsfnjk943RAeQ2Vcn9Ji9e/Z3Hf77984WRccO6zN42dMaH/R8hGDe7VbduyPn9WdL69tt/w3z3+gG/+5VFsqDrZ7rKPP+zfPf6Bnl23vrPopqW9wHYbkbz/8bkbLB4CuIiQnqSX0/vCfy2KOWNAyJmiifZIltbvqPVPiBfaV2/f5Us7Pn1qR1nJilt1Br4paH/rLBj5Oc5zlz63YkdFyCwq6/gbz9UKrE0YP0B2fO0FLfnye7v/yDM06aog+f/LodusN7RO7m8Tj78UeLOjP8zfq9y92vWV4zgMLtTqD3WTqm5pUXBR/a/9j8dbULqzNw4MuAJlFSE5SdEthrAtfWmaXOnShWHi+wX0b4SHOe163K7juDpJUHediJ7/K90O84pdvTe+BkFd0BPKO6YvYepcW64wJg3XPFz+kPqXFeuirJ2nO6eM0qFdzi/MxI/p0+TVfWtV+rPnxQ9q3SHud+5t5+tRdb2Tkwsvq2kaVFsW+cK/FlJ+kPrFI0MM8AsgdhOQoG3Yf0KJNh7ocvLBiR+uwTM45bas8dAozOmjGCiQtY5Te89p6vbp6V0ItyX5d4R2vLq+v2Z3WcoKcO6Cjz/vH/1qe8fLjvXe/fr47my49U0qKDn2lrPfhYCgXnXzEQN1wwSS9cd0s/fclx2rOGUfoJx89Ou760Z95R6aM7Kf515+tqSP7drjeOxv2asIPntKY657Ux+94Xf/vlbVpGRVmb3Wd+vco0YabL9SGmy+MuU5VbYNO+cULKZcFAOlQFHQFwmT2/8xTTX2TLv3QKI3o1123PveBpo7qp39eNVMPvb1ZNzy2RHO/dZomj+jTJgTFCiTRU7C+smpXQi2LzyzL7KnwFvHqMndJevsk+tViHbPsTj7vvQfqWvuGZqT8gGcXixdqbntxja49L/4FXKk6aezA1ttLt1Zq6qj2F6IhMSVFBfrMh5pn77vilDG6bMbhWr/7gN4rq1CBme59bb0uO/FwPbN0u16LcYDbu1uR9nvOmgzv213/vPrU1vu1DY3ae6BeL63aqesfXdLuNd7dVKF3N1XoF0+tlCR9aeYYXXnqWI3s36PL76eiul6jB3b+vK2VNRpz3ZM6Z9IQ/fGKD3W5HABIF0JylJr65mD78DuHxix+b3OFTrvlRRUVNLfWrN65X5NH9GkzqsUD8zfql59qGwaiQ3JxUUHCnS3qGpoSbhlKVtBdAfzQ2Xu8+qFFevArJ2Ws/KBn/Ouo5a+mvlGlccar7Ugiu010n+T/fGypPndi+z62SE5JUYEmDuuticN6S5I+ecJISdLF00Zoe2WNRvXvob++vUnTR/fXTU8s17fPmaBnl2/Xn+dv1CXHj4z5mt2KCjWsb6Eum3G4LptxuJxz2rGvVve9sUF3vdL+wsA/vb5Bf3p9gyTphNH9NXpADw3tW6qBPUs0oGeJhvYpVe/SIpUWF8ok/e2dzdqxv1ZfmjlG2/fV6IQx/Vtf64cXTdZNT8Q/q/P8ip0ac92TkqTepUW643PHa8bYAerWSZcNAEgXQnICNu851M2iMBICok9b/+/CMv3yU1PbPKe+oW13jERz6XV/f1+3fmZaCrXtXEdVaWpySV18FfO1AgzjnZX8+ppyVdU26L7X1+sbZ45v3a7pEn90C390FJKP+uHTeu9H58UdhixVc04fp7s90yev331AQ3p3U89ufOWkW5/SYvUpbd6WV546VpL0f984RZJ06pGD9NOLj4n7XC8z07C+pbru/KN03flHtS5vanLauKdaizfv1XubK7Wl4qD2HKjTW+v3aMe+mtbx4L0KTOpZUqR/RS4yPG38oNbHrjx1bGt9nXNas7NK89eV68Z/tu92tr+mQZ+/521J0rhBPdU/Esp7dytS/54l6lNarLfW+zNRD4D8wS9WF7VM9drZ2fS6qJZk14VRkh99d4t+/empGW1x7Khf6s79tRoWZ9KBrvJrdrtYEin6lqdX6s/zN2rMoJ66aMqINJcfuwLnTB6a1nLiiRdaWuw9UNflkJzo1vzOORPahORlWyt14W2vacrIvno8cqrfOafdVbVdKh/BKSgwjR3UU2MH9dTHj2vbKt3U5LS/pkG7qmq0a3+dqusadLC+UTX1TTphdH/1KS3S7S+t1eEDumv2McNivr6Z6cihvXXk0N76wsljWpfvr6nX8q379MyyHToQ6TpSebBe+2rqtam8WlW1DdpzoK51dsGSwgINzGA3KgD5hZDcRUu3VGrC0N5tulPEEv34/3tlnWaOH9jB2m0t3LhX08cMSLqOnekoP722ZnfradxUBRmSvQcC4wb3bDeqRlVN849uSzebdIo/zJ4/M9E1djKzYCa3TPeSQ6fD1+6q0oW3vSZJer+ssnX5Iws26/75GzNYC/iloMDUt0ex+vYo1vghsde58SOTk3rt3qXFOnHcQJ04ruPvz4bGJh2sb1RhgalHCT9rANKD0S266P75G3Xx7a+3G2v36r8uanPfG6Kju2x05pN3zU++ggnoqBvEU0u2pa2ceLMRSsr4rHfe9/jNs8e3W6eltT4Tk4vEa62f90H7obkyobOW5GQkMzLGc8tjX4z60kp/Pgfkh6LCAvUuLSYgA0grQnKSvOOIPvF+23BZ75lVrirG2Lxv3zBL0+Jc/Z/Jobo6amRMZ6kdtSS/s35PGktqz/seDx/Q/qr6lh4tf85Ai2bAg1t0OrrG4s1pnl3R4+qzmg9Kbo6MitBi3PVPqrahMWP9oQEASBdCckQ6Qmltw6GZ97whes+BunbrD+lTqn9cNVMrfjq73WNjr5+bcn3i6agl+cWVO9u8j5TK6SCMf+/v76eljLhle97jCaMH6Plrz2izLJMX0QU9gkhnLcm/e2FNRsu/6qz2LfdS88HD9Y8u0d8WbI75OAAAYUFIjkhHv9Tr/n5onNHOpj6+/bPHt97uXlKo5T/9cLt10j1NdIvOAly6ToU3dNIvdvOe6rSUE0usVmzvjGOZjLEdtaLHOqvgZ/mStC7D3V2i+yV7PbpoS0bLBgAgHQjJEQfqUg8uL6w41P/yYAev9/y1p+vCKcPbLOtRUqQ1/3V+m24Bs3/7akYufvN2BfH676dXdvh4ojoL46fd8lJayokl3ud2fdSwVnujWvfTPQ1vR9vtI797La1lxZKZPsldW39MAhNHSEwjDAAIJ0JyRHVt6l0M9tU0qLHJyTmn6vr2rze0Tze9fcMsjR/SO+bziwoLNO97Z6mk8NBmOeKGuWmfva2zkTnSdVFdJ8VIap4KPBPihcQ5p49rvf3Cyp2tt1uG9st0+VLz57tmZ1Vay/MKcmSRFvd+sfPZ0nqUFKZ9jGoAANKBkBxx9UOLOl8pAf9cvEW1DU0xW92G9e2uIX06H4P4A0+L8rgb0ts/uaGTlmSp+X2kKpGgduavXk65nK6UHa/VsqwivV0/Onvv59z6Suu4r5mQyGffMptZpowb3KvDxwf37qblMfrjAwAQBoTkCO8Yusm69pH30tLXdt73ztKPosYW/dE/l6b8mi06a0mWpGseXpxyOfGC2v9c2nZGwU3l6e+b3FF/6J99rP0MZBfe9lpaW19bym/p3nH5SYfr9evO1tfOONSSffSPnklbee3LT+y9/NuDCxN+zcSnxEnw9YJv7AYAIC5CckQ6z/ie+5t5sR/oYir40syx+lVkuuv752/Udx95T/tq6lOtXkIhWUr9wrq6xthdGD58dNtZt07/Zfr7Jnf0Fi8/abSmjOzbbvn8temb1raltf6E0f214eYL9bOPHavD+nXX9edP0mP/dkrreplqze1sMpEWc5ds1859NRmpgyR9+5wj4z72pwS6YwAAEBRCcoQf/SKTuUDpkyeM1NqfX6BvnT1ef19Upik/fla3PrsqpXp0NvJGi9NueSmlofHiXQzXrahA3zlnQptlb61LX0CVmkPi1FH9NLRPNz35rVPbPf741adq5U1tT/Vffs9baWtNbnmdWPvVcYf3128/c6g1/YwMHCR05cK9GT9/IaH1kvlovnX2kTp1/KB2y3968dE6NsaBCgAAYUFIjhjZP7Er8VOR7EX8hQWma8+b2NrydtuLa/Sdvy1O+qK3gzEuKownlfGa6+L0fTYzXeNpYfzM3W+mtVW1vtHphMP7660bztHRI2KHsdLiQt33pbatmUekqf93S2t9UUHsP7GPHXeYnvhmc3jfWF6tiT94Kq0TyDQ2uTYXgLa48aLY0wMnctYgmQOIggLTpOHtL1S9/MTRXX4tAAD8REiOuP2zx+uWS6boE8cfpkuOHylJmjC04wuP/HbWUUP07g/P1ZzTx+mJ97dq1q2v6JqH39WiTXu7FLBquhCSpeYuAcmMsBGrJfl/v35y6+0NN1+oB66c0a6sdExmUlPfqNLiznfvMycO0QvfbTvJyMKNqc9G19Ja31Edjjmsr5665rTW9b/50LuqTsNQhFLzaB1jB/XUf3x4op781qn6zwsmaf0vLtCXTx2rDTdfqN9/9rg26592y0vaWtHx1OmJXPAZy79/eKJuuvjoNssKGNECABByhOSIwwf20Kc/NEq3fnqafv3pqXrvxvP0xDdP03fPnaC/XHmijj+8X+vYxn1Ki5IqoyAN48H271miGy6YpHnfO0tfOmWMnl++Q5+44w2d+5t5+s1zH+j55Ts05ronNea6J1V5MHb/5Yrq9ssfv3qmvj/7qJgXtUnNI2w89m5Zl8J4y1jRXz1trP79vAlafOO5+tCYAW3WOe3IwVr/iwvaLJv4g6f117c2JVyOV0NjkxqanEqL409oEe2Iwb209CeHJnO55M43dMyPnkmp60XLgUhndZg0vI/W/vwC/ceHJ2rukm2a9etX0tI3+mB9o7qXFOqqs8br6BF99dXTx7Xp7nPRlBF678bz2jznlJtf1L/e2xr3NRPt5+zVrahQnz95jCYP7yNJ+uyJhyf1OgAA+MnSeYo3XaZPn+4WLFgQdDXaaWhsUlVtg7ZUHFRtQ5OKCkwH6xq1eHOFBvfupp8+sbxNAD2sX3dtiWqd+/T0kbrlk1PTWqeq2gY9+f5WPbKgTO9u2huz3+hz3zld44f0ag1J//nYEs1dsk2TR/TR62vK9eBXTtRMT79R55y++8h7evTd9kPBnTp+kP54xfROA+CdL6/Vfz+9Uit+OrvDGdik5lD5hXvf1tvr97RZfvG0Efr1p6aqKEbXgXiqaht0zI+e0Q0XHKU5px+R8PMkadavX9baqJFOfnDhJH3xlDFdKl+SHnhzo374j6V6+z9naUjvzof9k6Q315Xr2r8t1tbKGl00ZbiuOGWMpo/un1Rf9k/c8bq6lxTqwa+c1Om6+2rqNeXHz7ZZ9tYNszTUM1zhfz+9Une+vFZS81mAZFTVNqh7MWMjAwDCwcwWOuemx3yMkJx+WysOqsk5DerVTbv216ryYL02llfr7KOGdBoWU7G/pl7vba7U5fe81e6x0uICzTpqqE4+YqD+8uZGFRaYnvzWaQm97pKySn3k9+1niTtqWG9dNuNwnTC6vyYM7a2SorZB8mdPLNcDb27UyptmJxz0nHP67B/e0nzPhXwXTRmuWZOGaMbYgTqsX/cOX2NrxUGdcvOL+sUnjtVlM7rearlw4x5dcuf81vvD+pTq/GOH6ZxJQ3XC6P4JtVDf8fIa3fL0Kq28aXbCLdqSdKC2QXe+vFb3vr5e1XWNGj2why6eOkKnTRisqSP7tfuM4zntlhd1wuH99dtLj+t8ZUk79tXoxBgX8P36U1P1ieMPk5np53NX6O556/TAlTN02pGDE35PAACEFSE5DzU1Oa0vP6DbXlitBRv2qsk5bas8NNTXf3x4oq46a3yXX3fl9n368ePLtHbXAVVU17VOcV1caDpySG8dNby3hvUp1eDe3fTw25slSc985/Sk3sOu/bX63B/fVE19k/ZW12l/TXP3jeF9SzV5eB9NGNZbYwf21OiBPTRyQA8N7d1NRYUFraH+rstP0OxjhnVSSny1DY2657X1WrBhr15bs1t1DU3qVlSgScP7aNLwPpo8oo+OGNxTYwb21LA+pW362f7sieX6y1sbtfKm85Mq+0Btg55eul3/t7BMb64vl3PNBzpHj+irScN7a/SA5vc9emBPHT6gR5uDr6Ymp6NufFpfPGWMbrhgUpfK3XOgTsff9FybZX27F+vcyUO1aU+1VmzdpyVRXVMAAMhmhGRIar6Ya/m2Su2vadDpRw5O+eIp55w27anWe2WVWr51n5ZtrdSanVXatb+2dQiya8+doG/Nij9WbqIam5xWbt+nN9ft0ZKyCi3buk8byg+0hnSpeazrwb27qaHRqfxAnV769zM1dlDPlMuWmrsJvLWuXG+sLdeyrZVasW1/mz7fRQWmQb26aUifbiotLtT7ZRU6YnCvhFvrO1JZXa/568r11vpyLd1SqVXb92tfTdsL/PqUFql/zxL161GiQpMWbarQbz4zVR8/bmTS5b60aqd++9wHKi4s0OqdVao8WK/RA3volf84K9W3BABAKBCS4aumJqeKg/WqqmnQqAHdk+pTm4iGxiZtqTioDeXV2lpxUNsqa7S98qAqD9brxLED9eVTx2akXKn5AGFbZY027D6g9eUHtGXvQe3cX6ud+2tV19CoAjP925njdeqR7ccIToeK6jptLK/Wxj3V2rj7gHZX1Wpvdb32VtdpX02Djh7RRz/6yGR1K0pP956mJqcd+2vUrahQA3qWpOU1AQAIGiEZAAAA8OgoJDMEHAAAAOBBSAYAAAA8CMkAAACAR9Ih2cwmmtniqH/7zOzbnnXONLPKqHVuTL3KAAAAQGYlN7+yJOfcKknTJMnMCiVtkfRYjFVfdc5dlGw5AAAAgN/S1d1ilqS1zrmNaXo9AAAAIDDpCsmXSnoozmMnm9l7ZvaUmR0d7wXMbI6ZLTCzBbt27UpTtQAAAICuSzkkm1mJpI9K+t8YDy+SNNo5N1XS7yT9I97rOOfuds5Nd85NHzx4cKrVAgAAAJKWjpbk8yUtcs7t8D7gnNvnnKuK3J4rqdjMMjMFGQAAAJAm6QjJlylOVwszG2aROYnNbEakvPI0lAkAAABkTNKjW0iSmfWQdK6kr0Ut+7okOefukvRJSd8wswZJByVd6sI4DzYAAAAQJaWQ7JyrljTQs+yuqNu/l/T7VMoAAAAA/MaMewAAAIAHIRkAAADwICQDAAAAHoRkAAAAwIOQDAAAAHgQkgEAAAAPQjIAAADgQUgGAAAAPAjJAAAAgAchGQAAAPAgJAMAAAAehGQAAADAg5AMAAAAeBCSAQAAAA9CMgAAAOBBSAYAAAA8CMkAAACAByEZAAAA8CAkAwAAAB6EZAAAAMCDkAwAAAB4EJIBAAAAD0IyAAAA4EFIBgAAADwIyQAAAIAHIRkAAADwICQDAAAAHoRkAAAAwIOQDAAAAHgQkgEAAAAPQjIAAADgQUgGAAAAPFIKyWa2wcyWmNliM1sQ43Ezs9vMbI2ZvW9mx6dSHgAAAOCHojS8xlnOud1xHjtf0pGRfydKujPyPwAAABBame5ucbGkP7tmb0rqZ2bDM1wmAAAAkJJUQ7KT9KyZLTSzOTEeP0zS5qj7ZZFl7ZjZHDNbYGYLdu3alWK1AAAAgOSlGpJnOueOV3O3iqvM7HTP4xbjOS7WCznn7nbOTXfOTR88eHCK1QIAAACSl1JIds5tjfy/U9JjkmZ4VimTNCrq/khJW1MpEwAAAMi0pEOymfU0s94ttyWdJ2mpZ7XHJX0hMsrFSZIqnXPbkq4tAAAA4INURrcYKukxM2t5nb865542s69LknPuLklzJV0gaY2kaklfSq26AAAAQOYlHZKdc+skTY2x/K6o207SVcmWAQAAAASBGfcAAAAAD0IyAAAA4EFIBgAAADwIyQAAAIAHIRkAAADwICQDAAAAHoRkAAAAwIOQDAAAAHgQkgEAAAAPQjIAAADgQUgGAAAAPAjJAAAAgAchGQAAAPAgJAMAAAAehGQAAADAg5AMAAAAeBCSAQAAAA9CMgAAAOBBSAYAAAA8CMkAAACAByEZAAAA8CAkAwAAAB6EZAAAAMCDkAwAAAB4EJIBAAAAD0IyAAAA4EFIBgAAADwIyQAAAIAHIRkAAADwICQDAAAAHoRkAAAAwIOQDAAAAHgkHZLNbJSZvWRmK8xsmZldE2OdM82s0swWR/7dmFp1AQAAgMwrSuG5DZK+65xbZGa9JS00s+ecc8s9673qnLsohXIAAAAAXyXdkuyc2+acWxS5vV/SCkmHpatiAAAAQFDS0ifZzMZIOk7SWzEePtnM3jOzp8zs6A5eY46ZLTCzBbt27UpHtQAAAICkpBySzayXpL9L+rZzbp/n4UWSRjvnpkr6naR/xHsd59zdzrnpzrnpgwcPTrVaAAAAQNJSCslmVqzmgPygc+5R7+POuX3OuarI7bmSis1sUCplAgAAAJmWyugWJukeSSucc7fGWWdYZD2Z2YxIeeXJlgkAn2Ak+wAABgFJREFUAAD4IZXRLWZK+rykJWa2OLLsBkmHS5Jz7i5Jn5T0DTNrkHRQ0qXOOZdCmQAAAEDGJR2SnXOvSbJO1vm9pN8nWwYAAAAQBGbcAwAAADwIyQAAAIAHIRkAAADwICQDAAAAHoRkAAAAwIOQDAAAAHgQkgEAAAAPQjIAAADgQUgGAAAAPAjJAAAAgAchGQAAAPAgJAMAAAAehGQAAADAg5AMAAAAeBCSAQAAAA9CMgAAAOBBSAYAAAA8CMkAAACAByEZAAAA8CAkAwAAAB6EZAAAAMCDkAwAAAB4EJIBAAAAD0IyAAAA4EFIBgAAADwIyQAAAIAHIRkAAADwICQDAAAAHoRkAAAAwIOQDAAAAHgQkgEAAACPlEKymc02s1VmtsbMrovxuJnZbZHH3zez41MpDwAAAPBD0iHZzAol3S7pfEmTJV1mZpM9q50v6cjIvzmS7ky2PAAAAMAvqbQkz5C0xjm3zjlXJ+lhSRd71rlY0p9dszcl9TOz4SmUCQAAAGRcKiH5MEmbo+6XRZZ1dR0AAAAgVIpSeK7FWOaSWKd5RbM5au6SIUlVZrYqhbola5Ck3QGUC3+xnfMD2zn3sY3zA9s5PwS1nUfHeyCVkFwmaVTU/ZGStiaxjiTJOXe3pLtTqE/KzGyBc256kHVA5rGd8wPbOfexjfMD2zk/hHE7p9Ld4h1JR5rZWDMrkXSppMc96zwu6QuRUS5OklTpnNuWQpkAAABAxiXdkuycazCzqyU9I6lQ0r3OuWVm9vXI43dJmivpAklrJFVL+lLqVQYAAAAyK5XuFnLOzVVzEI5edlfUbSfpqlTK8Fmg3T3gG7ZzfmA75z62cX5gO+eH0G1na86xAAAAAFowLTUAAADgQUhW59NrI9zM7F4z22lmS6OWDTCz58xsdeT//lGPXR/Z1qvM7MNRy08wsyWRx24zs1hDGCIgZjbKzF4ysxVmtszMroksZ1vnCDMrNbO3zey9yDb+SWQ52zgHmVmhmb1rZk9E7rOdc4yZbYhsn8VmtiCyLGu2c96H5ASn10a43SdptmfZdZJecM4dKemFyH1Ftu2lko6OPOeOyD4gNU+bPkeHplL3viaC1SDpu865SZJOknRVZHuyrXNHraSznXNTJU2TNDsyMhLbODddI2lF1H22c246yzk3LWp4t6zZznkfkpXY9NoIMefcPEl7PIsvlnR/5Pb9kj4Wtfxh51ytc269mkdemWHN06X3cc7Nj1xw+ueo5yAEnHPbnHOLIrf3q/nH9TCxrXOGa1YVuVsc+efENs45ZjZS0oWS/hi1mO2cH7JmOxOSmTo7Vw1tGZM78v+QyPJ42/uwyG3vcoSQmY2RdJykt8S2zimRU/CLJe2U9Jxzjm2cm34r6XuSmqKWsZ1zj5P0rJkttOaZlaUs2s4pDQGXIxKeOhs5Id72Zj/IEmbWS9LfJX3bObevg65pbOss5JxrlDTNzPpJeszMjulgdbZxFjKziyTtdM4tNLMzE3lKjGVs5+ww0zm31cyGSHrOzFZ2sG7otjMtyV2YOhtZZUfkFI0i/++MLI+3vcsit73LESJmVqzmgPygc+7RyGK2dQ5yzlVIelnNfQ/ZxrllpqSPmtkGNXdxPNvM/iK2c85xzm2N/L9T0mNq7uKaNduZkJzY9NrIPo9LuiJy+wpJ/4xafqmZdTOzsWq+AODtyCmf/WZ2UuSq2S9EPQchENku90ha4Zy7NeohtnWOMLPBkRZkmVl3SedIWim2cU5xzl3vnBvpnBuj5t/cF51zl4vtnFPMrKeZ9W65Lek8SUuVRds577tbxJteO+BqoQvM7CFJZ0oaZGZlkn4k6WZJj5jZlZI2SfqUJEWmTn9E0nI1j5ZwVeT0riR9Q80jZXSX9FTkH8JjpqTPS1oS6bMqSTeIbZ1Lhku6P3JFe4GkR5xzT5jZfLGN8wF/y7llqJq7TEnNefOvzrmnzewdZcl2ZsY9AAAAwIPuFgAAAIAHIRkAAADwICQDAAAAHoRkAAAAwIOQDAAAAHgQkgEAAAAPQjIAAADgQUgGAAAAPP4/23A3SkVdW7kAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def train_one_epoch(model, criterion, optimizer, data_loader, device, epoch, n_epochs, verbose=False):\n",
    "    pbar = tqdm.tqdm(enumerate(data_loader, start=1), total=len(data_loader), disable=not verbose)\n",
    "    total_loss = 0\n",
    "    for i, (x, y) in pbar:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits, fnns_out = model.forward(x)\n",
    "        loss = criterion(logits, y, fnns_out, feature_penalty=0.0)\n",
    "        #print(loss.item)\n",
    "        total_loss += loss.item()\n",
    "        #print(total_loss)\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        pbar.set_description(f\"train ({epoch+1}/{n_epochs}) | loss = {total_loss:.5f}\")\n",
    "        \n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    return avg_loss\n",
    "\n",
    "history = []\n",
    "n_epochs = 5000 #2000 #1000\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model = model.train()\n",
    "    verbose = (epoch + 1) % 100 == 0\n",
    "    total_loss = train_one_epoch(model, criterion, optimizer, train_loader, device, epoch, n_epochs, verbose=verbose)\n",
    "    \n",
    "    history.append(total_loss)\n",
    "#     if verbose:\n",
    "#         print(f\"Epoch {epoch + 1} | Total Loss: {total_loss:.5f}\")  \n",
    "        \n",
    "    scheduler.step()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(np.array(history))\n",
    "plt.legend(['Loss'])\n",
    "plt.ylim([0,20])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(np.arange(1,K+1),model.get_submodule('combination_layer').out_weights[:, 0].detach().cpu().numpy())\n",
    "plt.title(f'Combination')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(x_train, y_train, x_validate, y_validate, device):\n",
    "    model = NeuralAdditiveModel(\n",
    "        input_size=x_train.shape[-1],\n",
    "        shallow_units=nam.data_utils.calculate_n_units(x_train, FLAGS.n_basis_functions, FLAGS.units_multiplier),\n",
    "        hidden_units=list(map(int, FLAGS.hidden_units)),\n",
    "        shallow_layer=ExULayer if FLAGS.shallow_layer == \"exu\" else ReLULayer,\n",
    "        hidden_layer=ExULayer if FLAGS.hidden_layer == \"exu\" else ReLULayer,\n",
    "        hidden_dropout=FLAGS.dropout,\n",
    "        feature_dropout=FLAGS.feature_dropout).to(device)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(),\n",
    "                                  lr=0.01,\n",
    "                                  weight_decay=0.01)\n",
    "    criterion = nam.metrics.penalized_mse if FLAGS.regression else nam.metrics.penalized_cross_entropy\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, gamma=0.995, step_size=1)\n",
    "\n",
    "#     train_dataset = TensorDataset(torch.tensor(x_train), torch.tensor(y_train))\n",
    "#     train_loader = DataLoader(train_dataset, batch_size=FLAGS.batch_size, shuffle=True)\n",
    "#     validate_dataset = TensorDataset(torch.tensor(x_validate), torch.tensor(y_validate))\n",
    "#     validate_loader = DataLoader(validate_dataset, batch_size=FLAGS.batch_size, shuffle=True)\n",
    "\n",
    "#    n_tries = FLAGS.early_stopping_epochs\n",
    "#    best_validation_score, best_weights = 0, None\n",
    "\n",
    "    for epoch in range(5000):\n",
    "        model = model.train()\n",
    "        total_loss = train_one_epoch(model, criterion, optimizer, train_loader, device)\n",
    "        logging.info(f\"epoch {epoch} | train | {total_loss=}\")\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "#         model = model.eval()\n",
    "#         metric, val_score = evaluate(model, validate_loader, device)\n",
    "#         logging.info(f\"epoch {epoch} | validate | {metric}={val_score}\")\n",
    "\n",
    "#         # early stopping\n",
    "#         if val_score <= best_validation_score and n_tries > 0:\n",
    "#             n_tries -= 1\n",
    "#             continue\n",
    "#         elif val_score <= best_validation_score:\n",
    "#             logging.info(f\"early stopping at epoch {epoch}\")\n",
    "#             break\n",
    "#         best_validation_score = val_score\n",
    "#         best_weights = copy.deepcopy(model.state_dict())\n",
    "\n",
    "#     model.load_state_dict(best_weights)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_one_epoch(model, criterion, optimizer, data_loader, device):\n",
    "    pbar = tqdm.tqdm(enumerate(data_loader, start=1), total=len(data_loader))\n",
    "    total_loss = 0\n",
    "    for i, (x, y) in pbar:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits, fnns_out = model.forward(x)\n",
    "        loss = criterion(logits, y, fnns_out, feature_penalty=FLAGS.output_regularization)\n",
    "        total_loss -= (total_loss / i) - (loss.item() / i)\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        pbar.set_description(f\"train | loss = {total_loss:.5f}\")\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of NeuralAdditiveModel(\n",
       "  (feature_nns): ModuleList(\n",
       "    (0): FeatureNN(\n",
       "      (shallow_layer): ExULayer()\n",
       "      (hidden_layers): ModuleList(\n",
       "        (0): ReLULayer()\n",
       "        (1): ReLULayer()\n",
       "      )\n",
       "      (dropout): Dropout(p=0.5, inplace=False)\n",
       "      (output_layer): Linear(in_features=4, out_features=1, bias=False)\n",
       "    )\n",
       "    (1): FeatureNN(\n",
       "      (shallow_layer): ExULayer()\n",
       "      (hidden_layers): ModuleList(\n",
       "        (0): ReLULayer()\n",
       "        (1): ReLULayer()\n",
       "      )\n",
       "      (dropout): Dropout(p=0.5, inplace=False)\n",
       "      (output_layer): Linear(in_features=4, out_features=1, bias=False)\n",
       "    )\n",
       "    (2): FeatureNN(\n",
       "      (shallow_layer): ExULayer()\n",
       "      (hidden_layers): ModuleList(\n",
       "        (0): ReLULayer()\n",
       "        (1): ReLULayer()\n",
       "      )\n",
       "      (dropout): Dropout(p=0.5, inplace=False)\n",
       "      (output_layer): Linear(in_features=4, out_features=1, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (feature_dropout): Dropout(p=0.2, inplace=False)\n",
       ")>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ModuleList' object has no attribute 'hidden_layers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-109-40e6a1369375>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# Print the model architecture\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mNAM_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_submodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'feature_nns'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden_layers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1183\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1184\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1185\u001b[1;33m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0m\u001b[0;32m   1186\u001b[0m             type(self).__name__, name))\n\u001b[0;32m   1187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'ModuleList' object has no attribute 'hidden_layers'"
     ]
    }
   ],
   "source": [
    "# Instantiate the network\n",
    "NAM_model = NeuralAdditiveModel(input_size=3,\n",
    "                 shallow_units=shallow_units,\n",
    "                 hidden_units=hidden_units,\n",
    "                 shallow_layer = ExULayer,\n",
    "                 hidden_layer = ReLULayer,\n",
    "                 feature_dropout = 0.2,\n",
    "                 hidden_dropout = 0.5,\n",
    "                 )\n",
    "\n",
    "# Print the model architecture\n",
    "NAM_model.get_submodule('feature_nns').hidden_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "tensor([[1.9531],\n",
      "        [2.7266]], grad_fn=<MmBackward0>)\n",
      "tensor([[5.3059],\n",
      "        [4.5475]], grad_fn=<MmBackward0>)\n",
      "tensor([[0.0000],\n",
      "        [0.0396]], grad_fn=<MmBackward0>)\n",
      "final output tensor([2.0515, 0.0000], grad_fn=<AddBackward0>)\n",
      "f_out tensor([[2.0515, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, -0.0000]], grad_fn=<MulBackward0>)\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "          ExULayer-1                    [-1, 5]               6\n",
      "         ReLULayer-2                    [-1, 7]              40\n",
      "           Dropout-3                    [-1, 7]               0\n",
      "         ReLULayer-4                    [-1, 4]              35\n",
      "           Dropout-5                    [-1, 4]               0\n",
      "            Linear-6                    [-1, 1]               4\n",
      "         FeatureNN-7                    [-1, 1]               0\n",
      "          ExULayer-8                    [-1, 5]               6\n",
      "         ReLULayer-9                    [-1, 7]              40\n",
      "          Dropout-10                    [-1, 7]               0\n",
      "        ReLULayer-11                    [-1, 4]              35\n",
      "          Dropout-12                    [-1, 4]               0\n",
      "           Linear-13                    [-1, 1]               4\n",
      "        FeatureNN-14                    [-1, 1]               0\n",
      "         ExULayer-15                    [-1, 5]               6\n",
      "        ReLULayer-16                    [-1, 7]              40\n",
      "          Dropout-17                    [-1, 7]               0\n",
      "        ReLULayer-18                    [-1, 4]              35\n",
      "          Dropout-19                    [-1, 4]               0\n",
      "           Linear-20                    [-1, 1]               4\n",
      "        FeatureNN-21                    [-1, 1]               0\n",
      "         ExULayer-22                    [-1, 5]               6\n",
      "        ReLULayer-23                    [-1, 7]              40\n",
      "          Dropout-24                    [-1, 7]               0\n",
      "        ReLULayer-25                    [-1, 4]              35\n",
      "          Dropout-26                    [-1, 4]               0\n",
      "           Linear-27                    [-1, 1]               4\n",
      "        FeatureNN-28                    [-1, 1]               0\n",
      "         ExULayer-29                    [-1, 5]               6\n",
      "        ReLULayer-30                    [-1, 7]              40\n",
      "          Dropout-31                    [-1, 7]               0\n",
      "        ReLULayer-32                    [-1, 4]              35\n",
      "          Dropout-33                    [-1, 4]               0\n",
      "           Linear-34                    [-1, 1]               4\n",
      "        FeatureNN-35                    [-1, 1]               0\n",
      "         ExULayer-36                    [-1, 5]               6\n",
      "        ReLULayer-37                    [-1, 7]              40\n",
      "          Dropout-38                    [-1, 7]               0\n",
      "        ReLULayer-39                    [-1, 4]              35\n",
      "          Dropout-40                    [-1, 4]               0\n",
      "           Linear-41                    [-1, 1]               4\n",
      "        FeatureNN-42                    [-1, 1]               0\n",
      "          Dropout-43                    [-1, 3]               0\n",
      "================================================================\n",
      "Total params: 510\n",
      "Trainable params: 510\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.00\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "# Assuming your input size is a single feature\n",
    "input_size = (3,)\n",
    "\n",
    "# Print a detailed summary of the model\n",
    "summary(NAM_model, input_size=input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating synthetic data using a normal distribution\n",
    "x1 = torch.normal(0, 1, (500,))\n",
    "x2 = torch.normal(0, 1, (500,))\n",
    "x3 = torch.normal(0, 1, (500,))\n",
    "y = 4 * x1 + x2**2 + x3**3\n",
    "\n",
    "x_data = torch.stack([x1, x2, x3], dim=1)\n",
    "y_data = y + torch.randn_like(y) * 0.1  # Adding some noise\n",
    "\n",
    "x_train.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
