{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Union, Iterable, Sized, Tuple\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.uniform import Uniform\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import logging\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchinfo import summary\n",
    "\n",
    "from data_processing.data_loader import *\n",
    "from model.activation_layers import ExULayer, ReLULayer, LipschitzMonotonicLayer\n",
    "from model.model_network import HierarchNeuralAdditiveModel,NeuralAdditiveModel\n",
    "from utils.model_architecture_type import get_defult_architecture_phase1, get_defult_architecture_phase2\n",
    "from training.trainer import Trainer\n",
    "from utils.utils import define_device, seed_everything\n",
    "from utils.model_parser import parse_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"nam_wandb.ipynb\"\n",
    "\n",
    "# import wandb\n",
    "# wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Ensure deterministic behavior\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Data Loading and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 10])\n",
      "torch.Size([3, 4])\n",
      "torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "def get_synthetic_data_phase1(num_exp=10, in_features=10):\n",
    "#     # Simulate independent variables, x0,...,x4 from a Uniform distribution on [âˆ’1, 1]\n",
    "#     X_uniform = Uniform(0, 3).sample((num_exp, in_features-5))\n",
    "    \n",
    "#     # Simulate features x5 and x6 from a Normal distribution with mean=0 and std=1\n",
    "#     X_normal = torch.normal(1.5, 0.5, (num_exp, 5))\n",
    "    \n",
    "#     # Combine the uniform and normal features\n",
    "#     X = torch.cat([X_uniform, X_normal], dim=1)\n",
    "    X = Uniform(0, 3).sample((num_exp, in_features))\n",
    "    print(X.shape)\n",
    "    \n",
    "    # creating y_1\n",
    "    # y_1 = X[:, 0] + 0.5*(3*(X[:, 1]**2)-1) + 0.5*(X[:, 2]**3)\n",
    "    y_1 = X[:, 0] + 2*(X[:, 1]**2) + (1/3)*(X[:, 2]**3)\n",
    "    y_1 = y_1.reshape(-1, 1)\n",
    "    #print(y_1.shape)\n",
    "    \n",
    "    # creating y_2\n",
    "    y_2 = (2/3) * torch.log(100 * X[:, 5].abs()) + torch.cos(5 * X[:, 6])\n",
    "    y_2 = y_2.reshape(-1, 1)\n",
    "    #print(y_2.shape)\n",
    "    \n",
    "    # creating y_3\n",
    "    y_3 = (3/4) * torch.exp(-4 * X[:, 7].abs()) + 0.5*(torch.sin(5 * X[:, 8])+1)\n",
    "    y_3 = y_3.reshape(-1, 1)\n",
    "    \n",
    "    # creating y_4\n",
    "    y_4 = torch.exp(0.5 * X[:, 5]) + 0.5*(X[:, 2]**2)\n",
    "    y_4 = y_4.reshape(-1, 1)\n",
    "    \n",
    "    # Stack all y_i to form the final target matrix\n",
    "    y = torch.cat([y_1, y_2, y_3, y_4], dim=1)\n",
    "    print(y.shape)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "\n",
    "def get_synthetic_data_phase2(X_input):\n",
    "    \n",
    "    # creating y_1\n",
    "    y_1 = - X_input[:, 0] - 2*X_input[:, 2]\n",
    "    y_1 = y_1.reshape(-1, 1)\n",
    "    #print(y_1.shape)\n",
    "    \n",
    "    # creating y_2\n",
    "    y_2 = 3*X_input[:, 1]\n",
    "    y_2 = y_2.reshape(-1, 1)\n",
    "    #print(y_2.shape)\n",
    "    \n",
    "    # Stack all y_i to form the final target matrix\n",
    "    y = torch.cat([y_1, y_2], dim=1)\n",
    "    print(y.shape)\n",
    "    \n",
    "    return y\n",
    "\n",
    "\n",
    "def make_loader(X, y, batch_size):\n",
    "    dataset = TensorDataset(torch.tensor(X), torch.tensor(y))\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    return loader\n",
    "\n",
    "\n",
    "X, y_phase1 = get_synthetic_data_phase1(num_exp=3, in_features=10)\n",
    "y_phase2 = get_synthetic_data_phase2(y_phase1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function for regression\n",
    "def feature_loss(fnn_out, lambda_=0.):\n",
    "    return lambda_ * (fnn_out ** 2).sum() / fnn_out.shape[1]\n",
    "\n",
    "def penalized_mse(logits, truth, fnn_out, feature_penalty=0.0):\n",
    "    feat_loss = feature_loss(fnn_out, feature_penalty)\n",
    "    mse_loss = F.mse_loss(logits.view(-1), truth.view(-1))\n",
    "    #print('mse_loss:', mse_loss)\n",
    "    #print('feat_loss:',feat_loss)\n",
    "    loss = mse_loss+feat_loss\n",
    "    return loss\n",
    "\n",
    "def l1_penalty(params, l1_lambda):\n",
    "    l1_norm =  torch.stack([torch.linalg.norm(p, 1) for p in params], dim=0).sum()\n",
    "    return l1_lambda*l1_norm\n",
    "\n",
    "def l2_penalty(params, l1_lambda):\n",
    "    l2_norm =  torch.stack([torch.linalg.norm(p, 2) for p in params], dim=0).sum()\n",
    "    return l1_lambda*l2_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Training Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, criterion, optimizer, config):\n",
    "    \n",
    "    # Run training and track with wandb\n",
    "#    total_batches = len(loader) * config['epochs']\n",
    "#    batch_ct = 0\n",
    "    loss_history = []\n",
    "\n",
    "    for epoch in tqdm(range(config['epochs'])):\n",
    "        epoch_loss_history = []\n",
    "        for _, (X, y) in enumerate(loader):\n",
    "            loss = train_batch(X, y, model, optimizer, criterion)\n",
    "#            batch_ct += 1\n",
    "            epoch_loss_history.append(loss)\n",
    "#             # Report metrics every 5th batch\n",
    "#             if ((batch_ct + 1) % 5) == 0:\n",
    "#                 print(f\"Epoch {epoch} | Batch {batch_ct + 1} | Total Loss: {loss:.5f}\")\n",
    "\n",
    "        avg_loss = sum(epoch_loss_history) / len(loader)\n",
    "        loss_history.append(avg_loss)\n",
    "    \n",
    "        if epoch%100==0:\n",
    "            print(f\"Epoch {epoch} | Total Loss: {avg_loss:.5f}\")\n",
    "        \n",
    "    return loss_history\n",
    "        \n",
    "def train_batch(X, y, model, optimizer, criterion):    \n",
    "    X, y = X.to(device), y.to(device)\n",
    "    # Forward pass\n",
    "    logits, phase1_gams_out, phase2_gams_out = model(X)\n",
    "    loss = criterion(logits, y, phase2_gams_out, feature_penalty=0.0002)\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # Step with optimizer\n",
    "    optimizer.step()\n",
    "    \n",
    "    if 0:\n",
    "        print('predict outputs shape:',logits.shape)\n",
    "        print('true outputs shape:',y.shape)\n",
    "        print('fnns_out shape:',fnns_out.shape)\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define experiment\n",
    "def define_experiment(ActivateLayers_pase1='ReLU', ActivateLayers_pase2=None):\n",
    "    config = dict(\n",
    "                epochs=2000,#2000, #100\n",
    "                batch_size=1024, #128\n",
    "                learning_rate=0.0035,\n",
    "                weight_decay=0.0001,\n",
    "                num_exp = 50000,\n",
    "                in_features = 10,    \n",
    "                seed=42,\n",
    "                hierarch_net=True,\n",
    "                task_type=\"regression\",\n",
    "                dataset=\"gen_uniform_distribution_data\",\n",
    "                model_architecture=\"multi_output_NAM\"\n",
    "                )\n",
    "    \n",
    "    if ActivateLayers_pase2 is not None:\n",
    "        hirarchical_net = [ActivateLayers_pase1, ActivateLayers_pase2]\n",
    "    else:\n",
    "        hirarchical_net = [ActivateLayers_pase1]\n",
    "    config['hirarchical_net'] = hirarchical_net\n",
    "\n",
    "    for phase in range(len(hirarchical_net)):\n",
    "        if hirarchical_net[phase] == 'ReLU':\n",
    "            config[f'first_ActivateLayer_phase{phase+1}'] = 'ReLU'\n",
    "            config[f'first_hidden_dim_phase{phase+1}'] = 64           \n",
    "            config[f'shallow_phase{phase+1}'] = False\n",
    "            config[f'hidden_ActivateLayer_phase{phase+1}'] = 'ReLU'\n",
    "            config[f'hidden_dim_phase{phase+1}'] = [64, 32]\n",
    "            config[f'hidden_dropout_phase{phase+1}'] = 0.2\n",
    "            config[f'feature_dropout_phase{phase+1}'] = 0.3\n",
    "            config[f'featureNN_arch_phase{phase+1}'] = 'multi_output' #'single_to_multi_output'\n",
    "            config[f'weight_norms_kind_phase{phase+1}'] = None\n",
    "            config[f'group_size_phase{phase+1}'] = None\n",
    "            config[f'monotonic_constraint_phase{phase+1}'] = None\n",
    "            if phase==0:\n",
    "                config['latent_dim'] = 4\n",
    "            else:\n",
    "                config['output_dim'] = 2\n",
    "\n",
    "        elif hirarchical_net[phase] == 'ExU':\n",
    "            config[f'first_ActivateLayer_phase{phase+1}'] = 'ExU'\n",
    "            config[f'first_hidden_dim_phase{phase+1}'] = 1024           \n",
    "            config[f'shallow_phase{phase+1}'] = True\n",
    "            config[f'hidden_dropout_phase{phase+1}'] = 0.4\n",
    "            config[f'feature_dropout_phase{phase+1}'] = 0.2\n",
    "            config[f'featureNN_arch_phase{phase+1}'] = 'multi_output'\n",
    "            config[f'weight_norms_kind_phase{phase+1}'] = None\n",
    "            config[f'group_size_phase{phase+1}'] = None\n",
    "            config[f'monotonic_constraint_phase{phase+1}'] = None\n",
    "            if phase==0:\n",
    "                config['latent_dim'] = 4\n",
    "            else:\n",
    "                config['output_dim'] = 2\n",
    "\n",
    "        elif hirarchical_net[phase] == 'LipschitzMonotonic':\n",
    "            config[f'first_ActivateLayer_phase{phase+1}'] = 'LipschitzMonotonic'\n",
    "            config[f'first_hidden_dim_phase{phase+1}'] = 128           \n",
    "            config[f'shallow_phase{phase+1}'] = False\n",
    "            config[f'hidden_ActivateLayer_phase{phase+1}'] = 'LipschitzMonotonic'\n",
    "            config[f'hidden_dim_phase{phase+1}'] = [128, 64]\n",
    "            config[f'hidden_dropout_phase{phase+1}'] = 0\n",
    "            config[f'feature_dropout_phase{phase+1}'] = 0\n",
    "            config[f'featureNN_arch_phase{phase+1}'] = 'multi_output'\n",
    "            config[f'weight_norms_kind_phase{phase+1}'] = \"one-inf\"\n",
    "            config[f'group_size_phase{phase+1}'] = 2\n",
    "            config[f'monotonic_constraint_phase{phase+1}'] = None\n",
    "            if phase==0:\n",
    "                config['latent_dim'] = 4\n",
    "            else:\n",
    "                config['output_dim'] = 2\n",
    "                \n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make(config):\n",
    "    \n",
    "    seed_everything(seed=config['seed'])\n",
    "    \n",
    "    # Make the data\n",
    "    X, y = get_synthetic_data_phase1(num_exp=config['num_exp'], in_features=config['in_features'])\n",
    "    Y = get_synthetic_data_phase2(y)\n",
    "\n",
    "    # Make the model\n",
    "    if len(config['hirarchical_net']) == 2:\n",
    "        print(\"Training Hierarchical NAM...\")\n",
    "        print(f\"Phase1 activation layers: {config['hirarchical_net'][0]}\\nPhase2 activation layers: {config['hirarchical_net'][1]}\")\n",
    "        \n",
    "        train_loader = make_loader(X, Y, batch_size=config['batch_size'])\n",
    "        \n",
    "        model = HierarchNeuralAdditiveModel(\n",
    "                            num_inputs= config['in_features'],\n",
    "                            task_type= config['task_type'],\n",
    "                            hierarch_net= config['hierarch_net'],\n",
    "                      #phase1 - latent_features:\n",
    "                            num_units_phase1= config['first_hidden_dim_phase1'],\n",
    "                            hidden_units_phase1= config['hidden_dim_phase1'],\n",
    "                            hidden_dropout_phase1 = config['hidden_dropout_phase1'],\n",
    "                            feature_dropout_phase1 = config['feature_dropout_phase1'],\n",
    "                            shallow_phase1 = config['shallow_phase1'],     \n",
    "                            first_layer_phase1 = config['first_ActivateLayer_phase1'],\n",
    "                            hidden_layer_phase1 = config['hidden_ActivateLayer_phase1'],\n",
    "                            latent_var_dim = config['latent_dim'],\n",
    "                            featureNN_architecture_phase1 = config['featureNN_arch_phase1'],\n",
    "                            weight_norms_kind_phase1 = config['weight_norms_kind_phase1'],\n",
    "                            group_size_phase1 = config['group_size_phase1'],\n",
    "                            monotonic_constraint_phase1 = config['monotonic_constraint_phase1'],\n",
    "                      #phase2 - final outputs:\n",
    "                            num_units_phase2 = config['first_hidden_dim_phase2'],\n",
    "                            hidden_units_phase2 = config['hidden_dim_phase2'],\n",
    "                            hidden_dropout_phase2 = config['hidden_dropout_phase2'],\n",
    "                            feature_dropout_phase2 = config['feature_dropout_phase2'],\n",
    "                            shallow_phase2 = config['shallow_phase2'],\n",
    "                            first_layer_phase2 = config['first_ActivateLayer_phase2'],\n",
    "                            hidden_layer_phase2 = config['hidden_ActivateLayer_phase2'],        \n",
    "                            output_dim = config['output_dim'],\n",
    "                            featureNN_architecture_phase2 = config['featureNN_arch_phase2'],\n",
    "                            weight_norms_kind_phase2 = config['weight_norms_kind_phase2'],\n",
    "                            group_size_phase2 = config['group_size_phase2'],\n",
    "                            monotonic_constraint_phase2 = config['monotonic_constraint_phase2'],\n",
    "                            ).to(device)\n",
    "    else:\n",
    "        print('Training NAM...')\n",
    "        print(f\"Activation layers: {config['hirarchical_net'][0]}\")\n",
    "        \n",
    "        train_loader = make_loader(X, y, batch_size=config['batch_size'])\n",
    "        \n",
    "        model = NeuralAdditiveModel(num_inputs= config['in_features'],\n",
    "                     num_units= config['first_hidden_dim_phase1'],\n",
    "                     hidden_units= config['hidden_dim_phase1'],\n",
    "                     hidden_dropout = config['hidden_dropout_phase1'],\n",
    "                     feature_dropout = config['feature_dropout_phase1'],\n",
    "                     shallow = config['shallow_phase1'],     \n",
    "                     first_layer = config['first_ActivateLayer_phase1'],\n",
    "                     hidden_layer = config['hidden_ActivateLayer_phase1'],\n",
    "                     num_classes = config['latent_dim'],\n",
    "                     architecture_type = config['featureNN_arch_phase1'],\n",
    "                     weight_norms_kind = config['weight_norms_kind_phase1'],\n",
    "                     group_size = config['group_size_phase1'], \n",
    "                     monotonic_constraint = config['monotonic_constraint_phase1'],        \n",
    "                     ).to(device)\n",
    "    \n",
    "    # Make the loss and optimizer\n",
    "    criterion = penalized_mse\n",
    "    optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                 lr=config['learning_rate'],\n",
    "                                 weight_decay=config['weight_decay'],\n",
    "                                )\n",
    "    \n",
    "    #scheduler = torch.optim.lr_scheduler.StepLR(optimizer, gamma=0.995, step_size=1)\n",
    "    \n",
    "    return model, train_loader, criterion, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_gam(model, x_values, input_dim, output_dim, config, vis_lat_features = False):\n",
    "    # Plot learned functions\n",
    "    fig, axes = plt.subplots(input_dim, output_dim, figsize=(15,30))\n",
    "\n",
    "    feature_output_max = {} \n",
    "    feature_output_min = {}\n",
    "\n",
    "    for j in range(output_dim):\n",
    "        feature_output_max[f'output_{j}'] = []\n",
    "        feature_output_min[f'output_{j}'] = []\n",
    "\n",
    "    for i in range(input_dim):\n",
    "        with torch.no_grad():\n",
    "            feature_input = x_values\n",
    "            for j in range(output_dim):\n",
    "                if len(config['hirarchical_net']) == 2:\n",
    "                    if vis_lat_features:\n",
    "                        feature_output = model.NAM_features.feature_nns[i](feature_input[:, 0])[:, j].cpu().numpy()\n",
    "                    else:          \n",
    "                        feature_output = model.NAM_output.feature_nns[i](feature_input[:, 0])[:, j].cpu().numpy()\n",
    "                else:\n",
    "                    # plot without hirarchical model\n",
    "                    feature_output = model.feature_nns[i](feature_input[:, 0])[:, j].cpu().numpy()\n",
    "                    \n",
    "                feature_output_max[f'output_{j}'].append(max(feature_output)) \n",
    "                feature_output_min[f'output_{j}'].append(min(feature_output))\n",
    "\n",
    "    for i in range(input_dim):\n",
    "        with torch.no_grad(): \n",
    "            for j in range(output_dim):\n",
    "                ax1 = axes[i, j]\n",
    "                if len(config['hirarchical_net']) == 2:\n",
    "                    if vis_lat_features:\n",
    "                        feature_output = model.NAM_features.feature_nns[i](feature_input[:, 0])[:, j].cpu().numpy()\n",
    "                    else:          \n",
    "                        feature_output = model.NAM_output.feature_nns[i](feature_input[:, 0])[:, j].cpu().numpy()\n",
    "                else:\n",
    "                    # plot without hirarchical model\n",
    "                    feature_output = model.feature_nns[i](feature_input[:, 0])[:, j].cpu().numpy()\n",
    "                    \n",
    "                ax1.scatter(x_values.cpu().numpy(), feature_output, label=f'Feature {i+1}')\n",
    "                ax1.set_title(f'Feature {i+1} to output {j}')\n",
    "                ax1.set_xlabel('Input')\n",
    "                ax1.set_ylabel('Output')\n",
    "                ax1.set_ylim([min(feature_output_min[f'output_{j}'])*1.3, max(feature_output_max[f'output_{j}'])*1.3])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50000, 10])\n",
      "torch.Size([50000, 4])\n",
      "torch.Size([50000, 2])\n",
      "Training Hierarchical NAM...\n",
      "Phase1 activation layers: ReLU\n",
      "Phase2 activation layers: LipschitzMonotonic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-2fa201bc4093>:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dataset = TensorDataset(torch.tensor(X), torch.tensor(y))\n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 265884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/2000 [00:19<10:55:26, 19.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Total Loss: 45.22864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 11/2000 [03:12<9:40:13, 17.50s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-9dde99d83965>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m# Run the training phase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0mloss_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Repos/multi-classification-NAM/training/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, loader, val_loader)\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mval_loss_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m             \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m             \u001b[0mtrain_loss_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Repos/multi-classification-NAM/training/trainer.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m             \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Repos/multi-classification-NAM/training/trainer.py\u001b[0m in \u001b[0;36mtrain_batch\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_value\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m             \u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m                             )\n\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"differentiable\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    224\u001b[0m             )\n\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m             adam(\n\u001b[0m\u001b[1;32m    227\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mmaybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mdisabled_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_fallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    764\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 766\u001b[0;31m     func(\n\u001b[0m\u001b[1;32m    767\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_multi_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    600\u001b[0m                 \u001b[0mexp_avg_sq_sqrt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_foreach_sqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_max_exp_avg_sqs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m                 \u001b[0mexp_avg_sq_sqrt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_foreach_sqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_exp_avg_sqs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_foreach_div_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq_sqrt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias_correction2_sqrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ActivateLayers_pase1 ='ReLU' # ReLU, ExU, LipschitzMonotonic, ExU_ReLU\n",
    "ActivateLayers_pase2 ='LipschitzMonotonic' # ReLU, ExU, LipschitzMonotonic, ExU_ReLU\n",
    "\n",
    "config = define_experiment(ActivateLayers_pase1='ReLU', ActivateLayers_pase2='LipschitzMonotonic')\n",
    "\n",
    "# make the model, data, and optimization problem\n",
    "model, train_loader, criterion, optimizer = make(config)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Number of parameters: {total_params}\")\n",
    "\n",
    "if 0:\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f'{name}: {param.numel()} parameters')\n",
    "if 0:\n",
    "    print(summary(model,input_size=(1, 10)))\n",
    "\n",
    "# train the model\n",
    "# Initialize the Trainer class\n",
    "trainer = Trainer(\n",
    "        model=model,\n",
    "        optimizer='Adam',\n",
    "        loss_function=None,\n",
    "        lr_scheduler='NoScheduler', \n",
    "        scheduler_params=None,\n",
    "        eval_metric=None, \n",
    "        epochs=config['epochs'], \n",
    "        batch_size=config['batch_size'], \n",
    "        learning_rate=config['learning_rate'],\n",
    "        weight_decay=config['weight_decay'], \n",
    "        l1_lambda_phase1=0,\n",
    "        l1_lambda_phase2=0,\n",
    "        l2_lambda_phase1=0,\n",
    "        l2_lambda_phase2=0.0002,\n",
    "        eval_every=100,\n",
    "        device_name=\"auto\"\n",
    ")\n",
    "\n",
    "# Run the training phase\n",
    "loss_history = trainer.train(train_loader)\n",
    "\n",
    "\n",
    "\n",
    "#loss_history = train(model, train_loader, criterion, optimizer, config)\n",
    "# loss_history_np = [loss.detach().cpu().numpy() for loss in loss_history]\n",
    "\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.plot(loss_history_np)\n",
    "# plt.legend(['Loss'])\n",
    "# plt.show()\n",
    "\n",
    "# # Generate input values for plotting\n",
    "# x_values = torch.linspace(0, 4, 500).reshape(-1, 1)  # 100 points between -1 and 1\n",
    "\n",
    "# input_dim = config['latent_dim']\n",
    "# output_dim = config['output_dim']\n",
    "# visualize_gam(model, x_values, input_dim, output_dim, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
